{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.646988 M parameters\n",
      "num decayed parameter tensors: 50, with 1,631,744 parameters\n",
      "num non-decayed parameter tensors: 87, with 15,244 parameters\n",
      "using fused AdamW: True\n",
      "Epoch 1 | Loss: 1.336731 | LR: 8.3916e-07 | Grad Norm: 5.7405\n",
      "Epoch 2 | Loss: 1.323864 | LR: 1.6783e-06 | Grad Norm: 5.8269\n",
      "Epoch 3 | Loss: 1.306488 | LR: 2.5175e-06 | Grad Norm: 5.6404\n",
      "Epoch 4 | Loss: 1.273605 | LR: 3.3566e-06 | Grad Norm: 5.5828\n",
      "Epoch 5 | Loss: 1.242672 | LR: 4.1958e-06 | Grad Norm: 5.3012\n",
      "Epoch 6 | Loss: 1.193863 | LR: 5.0350e-06 | Grad Norm: 5.1110\n",
      "Epoch 7 | Loss: 1.153343 | LR: 5.8741e-06 | Grad Norm: 4.6403\n",
      "Epoch 8 | Loss: 1.112648 | LR: 6.7133e-06 | Grad Norm: 4.0861\n",
      "Epoch 9 | Loss: 1.097946 | LR: 7.5524e-06 | Grad Norm: 3.4275\n",
      "Epoch 10 | Loss: 1.036438 | LR: 8.3916e-06 | Grad Norm: 3.3261\n",
      "Epoch 11 | Loss: 1.032331 | LR: 9.2308e-06 | Grad Norm: 2.7882\n",
      "Epoch 12 | Loss: 1.016830 | LR: 1.0070e-05 | Grad Norm: 2.5367\n",
      "Epoch 13 | Loss: 0.996000 | LR: 1.0909e-05 | Grad Norm: 2.3798\n",
      "Epoch 14 | Loss: 0.965668 | LR: 1.1748e-05 | Grad Norm: 2.3852\n",
      "Epoch 15 | Loss: 0.911448 | LR: 1.2587e-05 | Grad Norm: 2.5182\n",
      "Epoch 16 | Loss: 0.930300 | LR: 1.3427e-05 | Grad Norm: 2.3690\n",
      "Epoch 17 | Loss: 0.956673 | LR: 1.4266e-05 | Grad Norm: 2.2426\n",
      "Epoch 18 | Loss: 0.941118 | LR: 1.5105e-05 | Grad Norm: 2.2752\n",
      "Epoch 19 | Loss: 1.059158 | LR: 1.5944e-05 | Grad Norm: 1.8374\n",
      "Epoch 20 | Loss: 0.895712 | LR: 1.6783e-05 | Grad Norm: 2.4043\n",
      "Epoch 21 | Loss: 0.971220 | LR: 1.7622e-05 | Grad Norm: 2.1014\n",
      "Epoch 22 | Loss: 0.951275 | LR: 1.8462e-05 | Grad Norm: 2.1414\n",
      "Epoch 23 | Loss: 0.855524 | LR: 1.9301e-05 | Grad Norm: 2.5309\n",
      "Epoch 24 | Loss: 0.878775 | LR: 2.0140e-05 | Grad Norm: 2.4042\n",
      "Epoch 25 | Loss: 0.918051 | LR: 2.0979e-05 | Grad Norm: 2.2314\n",
      "Epoch 26 | Loss: 0.922750 | LR: 2.1818e-05 | Grad Norm: 2.2274\n",
      "Epoch 27 | Loss: 0.821978 | LR: 2.2657e-05 | Grad Norm: 2.5715\n",
      "Epoch 28 | Loss: 0.830345 | LR: 2.3497e-05 | Grad Norm: 2.5314\n",
      "Epoch 29 | Loss: 0.825386 | LR: 2.4336e-05 | Grad Norm: 2.5319\n",
      "Epoch 30 | Loss: 0.819832 | LR: 2.5175e-05 | Grad Norm: 2.5287\n",
      "Epoch 31 | Loss: 0.801342 | LR: 2.6014e-05 | Grad Norm: 2.5697\n",
      "Epoch 32 | Loss: 0.832903 | LR: 2.6853e-05 | Grad Norm: 2.4405\n",
      "Epoch 33 | Loss: 0.853203 | LR: 2.7692e-05 | Grad Norm: 2.3532\n",
      "Epoch 34 | Loss: 0.825684 | LR: 2.8531e-05 | Grad Norm: 2.4335\n",
      "Epoch 35 | Loss: 0.838156 | LR: 2.9371e-05 | Grad Norm: 2.3479\n",
      "Epoch 36 | Loss: 0.811062 | LR: 3.0210e-05 | Grad Norm: 2.4348\n",
      "Epoch 37 | Loss: 0.779860 | LR: 3.1049e-05 | Grad Norm: 2.5131\n",
      "Epoch 38 | Loss: 0.760616 | LR: 3.1888e-05 | Grad Norm: 2.5579\n",
      "Epoch 39 | Loss: 0.818865 | LR: 3.2727e-05 | Grad Norm: 2.3404\n",
      "Epoch 40 | Loss: 0.785545 | LR: 3.3566e-05 | Grad Norm: 2.4199\n",
      "Epoch 41 | Loss: 0.816095 | LR: 3.4406e-05 | Grad Norm: 2.2869\n",
      "Epoch 42 | Loss: 0.814287 | LR: 3.5245e-05 | Grad Norm: 2.2797\n",
      "Epoch 43 | Loss: 0.816591 | LR: 3.6084e-05 | Grad Norm: 2.2355\n",
      "Epoch 44 | Loss: 0.768780 | LR: 3.6923e-05 | Grad Norm: 2.3585\n",
      "Epoch 45 | Loss: 0.760820 | LR: 3.7762e-05 | Grad Norm: 2.3585\n",
      "Epoch 46 | Loss: 0.758486 | LR: 3.8601e-05 | Grad Norm: 2.3525\n",
      "Epoch 47 | Loss: 0.751945 | LR: 3.9441e-05 | Grad Norm: 2.3493\n",
      "Epoch 48 | Loss: 0.755398 | LR: 4.0280e-05 | Grad Norm: 2.2965\n",
      "Epoch 49 | Loss: 0.749819 | LR: 4.1119e-05 | Grad Norm: 2.2961\n",
      "Epoch 50 | Loss: 0.671677 | LR: 4.1958e-05 | Grad Norm: 2.5072\n",
      "Epoch 51 | Loss: 0.719360 | LR: 4.2797e-05 | Grad Norm: 2.3199\n",
      "Epoch 52 | Loss: 0.697081 | LR: 4.3636e-05 | Grad Norm: 2.3562\n",
      "Epoch 53 | Loss: 0.658098 | LR: 4.4476e-05 | Grad Norm: 2.4433\n",
      "Epoch 54 | Loss: 0.721577 | LR: 4.5315e-05 | Grad Norm: 2.2118\n",
      "Epoch 55 | Loss: 0.626575 | LR: 4.6154e-05 | Grad Norm: 2.4672\n",
      "Epoch 56 | Loss: 0.721523 | LR: 4.6993e-05 | Grad Norm: 2.1491\n",
      "Epoch 57 | Loss: 0.696878 | LR: 4.7832e-05 | Grad Norm: 2.1844\n",
      "Epoch 58 | Loss: 0.700787 | LR: 4.8671e-05 | Grad Norm: 2.1293\n",
      "Epoch 59 | Loss: 0.603256 | LR: 4.9510e-05 | Grad Norm: 2.3835\n",
      "Epoch 60 | Loss: 0.578564 | LR: 5.0350e-05 | Grad Norm: 2.4166\n",
      "Epoch 61 | Loss: 0.593534 | LR: 5.1189e-05 | Grad Norm: 2.3236\n",
      "Epoch 62 | Loss: 0.559679 | LR: 5.2028e-05 | Grad Norm: 2.3925\n",
      "Epoch 63 | Loss: 0.571070 | LR: 5.2867e-05 | Grad Norm: 2.3068\n",
      "Epoch 64 | Loss: 0.538292 | LR: 5.3706e-05 | Grad Norm: 2.3679\n",
      "Epoch 65 | Loss: 0.570149 | LR: 5.4545e-05 | Grad Norm: 2.2252\n",
      "Epoch 66 | Loss: 0.515758 | LR: 5.5385e-05 | Grad Norm: 2.3512\n",
      "Epoch 67 | Loss: 0.537711 | LR: 5.6224e-05 | Grad Norm: 2.2168\n",
      "Epoch 68 | Loss: 0.530764 | LR: 5.7063e-05 | Grad Norm: 2.2080\n",
      "Epoch 69 | Loss: 0.572599 | LR: 5.7902e-05 | Grad Norm: 2.0671\n",
      "Epoch 70 | Loss: 0.516426 | LR: 5.8741e-05 | Grad Norm: 2.1582\n",
      "Epoch 71 | Loss: 0.572091 | LR: 5.9580e-05 | Grad Norm: 2.0007\n",
      "Epoch 72 | Loss: 0.563876 | LR: 6.0420e-05 | Grad Norm: 1.9597\n",
      "Epoch 73 | Loss: 0.558854 | LR: 6.1259e-05 | Grad Norm: 1.9399\n",
      "Epoch 74 | Loss: 0.534575 | LR: 6.2098e-05 | Grad Norm: 1.9509\n",
      "Epoch 75 | Loss: 0.449197 | LR: 6.2937e-05 | Grad Norm: 2.0981\n",
      "Epoch 76 | Loss: 0.476105 | LR: 6.3776e-05 | Grad Norm: 1.9685\n",
      "Epoch 77 | Loss: 0.509628 | LR: 6.4615e-05 | Grad Norm: 1.8658\n",
      "Epoch 78 | Loss: 0.487729 | LR: 6.5455e-05 | Grad Norm: 1.8530\n",
      "Epoch 79 | Loss: 0.576472 | LR: 6.6294e-05 | Grad Norm: 1.6556\n",
      "Epoch 80 | Loss: 0.422532 | LR: 6.7133e-05 | Grad Norm: 1.8750\n",
      "Epoch 81 | Loss: 0.531509 | LR: 6.7972e-05 | Grad Norm: 1.6217\n",
      "Epoch 82 | Loss: 0.502934 | LR: 6.8811e-05 | Grad Norm: 1.6602\n",
      "Epoch 83 | Loss: 0.371927 | LR: 6.9650e-05 | Grad Norm: 1.7815\n",
      "Epoch 84 | Loss: 0.381276 | LR: 7.0490e-05 | Grad Norm: 1.7372\n",
      "Epoch 85 | Loss: 0.454866 | LR: 7.1329e-05 | Grad Norm: 1.5709\n",
      "Epoch 86 | Loss: 0.435030 | LR: 7.2168e-05 | Grad Norm: 1.5439\n",
      "Epoch 87 | Loss: 0.285242 | LR: 7.3007e-05 | Grad Norm: 1.6995\n",
      "Epoch 88 | Loss: 0.291705 | LR: 7.3846e-05 | Grad Norm: 1.6314\n",
      "Epoch 89 | Loss: 0.304898 | LR: 7.4685e-05 | Grad Norm: 1.5522\n",
      "Epoch 90 | Loss: 0.306720 | LR: 7.5524e-05 | Grad Norm: 1.4770\n",
      "Epoch 91 | Loss: 0.282404 | LR: 7.6364e-05 | Grad Norm: 1.4467\n",
      "Epoch 92 | Loss: 0.286909 | LR: 7.7203e-05 | Grad Norm: 1.4106\n",
      "Epoch 93 | Loss: 0.341679 | LR: 7.8042e-05 | Grad Norm: 1.2848\n",
      "Epoch 94 | Loss: 0.288660 | LR: 7.8881e-05 | Grad Norm: 1.2990\n",
      "Epoch 95 | Loss: 0.290092 | LR: 7.9720e-05 | Grad Norm: 1.2191\n",
      "Epoch 96 | Loss: 0.261252 | LR: 8.0559e-05 | Grad Norm: 1.2003\n",
      "Epoch 97 | Loss: 0.215174 | LR: 8.1399e-05 | Grad Norm: 1.1515\n",
      "Epoch 98 | Loss: 0.222161 | LR: 8.2238e-05 | Grad Norm: 1.0918\n",
      "Epoch 99 | Loss: 0.279507 | LR: 8.3077e-05 | Grad Norm: 1.0154\n",
      "Epoch 100 | Loss: 0.233956 | LR: 8.3916e-05 | Grad Norm: 0.9771\n",
      "Epoch 101 | Loss: 0.289578 | LR: 8.4755e-05 | Grad Norm: 0.8966\n",
      "Epoch 102 | Loss: 0.279809 | LR: 8.5594e-05 | Grad Norm: 0.8420\n",
      "Epoch 103 | Loss: 0.297327 | LR: 8.6434e-05 | Grad Norm: 0.7869\n",
      "Epoch 104 | Loss: 0.213460 | LR: 8.7273e-05 | Grad Norm: 0.8067\n",
      "Epoch 105 | Loss: 0.218008 | LR: 8.8112e-05 | Grad Norm: 0.7448\n",
      "Epoch 106 | Loss: 0.214040 | LR: 8.8951e-05 | Grad Norm: 0.7145\n",
      "Epoch 107 | Loss: 0.229972 | LR: 8.9790e-05 | Grad Norm: 0.6524\n",
      "Epoch 108 | Loss: 0.227067 | LR: 9.0629e-05 | Grad Norm: 0.6322\n",
      "Epoch 109 | Loss: 0.224094 | LR: 9.1469e-05 | Grad Norm: 0.5837\n",
      "Epoch 110 | Loss: 0.132487 | LR: 9.2308e-05 | Grad Norm: 0.5788\n",
      "Epoch 111 | Loss: 0.194497 | LR: 9.3147e-05 | Grad Norm: 0.5302\n",
      "Epoch 112 | Loss: 0.199920 | LR: 9.3986e-05 | Grad Norm: 0.4857\n",
      "Epoch 113 | Loss: 0.117712 | LR: 9.4825e-05 | Grad Norm: 0.4940\n",
      "Epoch 114 | Loss: 0.221236 | LR: 9.5664e-05 | Grad Norm: 0.4356\n",
      "Epoch 115 | Loss: 0.102563 | LR: 9.6503e-05 | Grad Norm: 0.4280\n",
      "Epoch 116 | Loss: 0.251251 | LR: 9.7343e-05 | Grad Norm: 0.3795\n",
      "Epoch 117 | Loss: 0.252681 | LR: 9.8182e-05 | Grad Norm: 0.3598\n",
      "Epoch 118 | Loss: 0.238649 | LR: 9.9021e-05 | Grad Norm: 0.3503\n",
      "Epoch 119 | Loss: 0.119465 | LR: 9.9860e-05 | Grad Norm: 0.3499\n",
      "Epoch 120 | Loss: 0.082085 | LR: 1.0070e-04 | Grad Norm: 0.3236\n",
      "Epoch 121 | Loss: 0.193322 | LR: 1.0154e-04 | Grad Norm: 0.3031\n",
      "Epoch 122 | Loss: 0.103597 | LR: 1.0238e-04 | Grad Norm: 0.2979\n",
      "Epoch 123 | Loss: 0.193371 | LR: 1.0322e-04 | Grad Norm: 0.2745\n",
      "Epoch 124 | Loss: 0.094831 | LR: 1.0406e-04 | Grad Norm: 0.2640\n",
      "Epoch 125 | Loss: 0.289658 | LR: 1.0490e-04 | Grad Norm: 0.2590\n",
      "Epoch 126 | Loss: 0.096284 | LR: 1.0573e-04 | Grad Norm: 0.2362\n",
      "Epoch 127 | Loss: 0.234776 | LR: 1.0657e-04 | Grad Norm: 0.2853\n",
      "Epoch 128 | Loss: 0.063210 | LR: 1.0741e-04 | Grad Norm: 0.2081\n",
      "Epoch 129 | Loss: 0.080478 | LR: 1.0825e-04 | Grad Norm: 0.1962\n",
      "Epoch 130 | Loss: 0.070442 | LR: 1.0909e-04 | Grad Norm: 0.2022\n",
      "Epoch 131 | Loss: 0.182904 | LR: 1.0993e-04 | Grad Norm: 0.1900\n",
      "Epoch 132 | Loss: 0.154414 | LR: 1.1077e-04 | Grad Norm: 0.1879\n",
      "Epoch 133 | Loss: 0.178776 | LR: 1.1161e-04 | Grad Norm: 0.1797\n",
      "Epoch 134 | Loss: 0.209643 | LR: 1.1245e-04 | Grad Norm: 0.1842\n",
      "Epoch 135 | Loss: 0.076738 | LR: 1.1329e-04 | Grad Norm: 0.2053\n",
      "Epoch 136 | Loss: 0.055735 | LR: 1.1413e-04 | Grad Norm: 0.1560\n",
      "Epoch 137 | Loss: 0.073247 | LR: 1.1497e-04 | Grad Norm: 0.2473\n",
      "Epoch 138 | Loss: 0.065283 | LR: 1.1580e-04 | Grad Norm: 0.1715\n",
      "Epoch 139 | Loss: 0.070494 | LR: 1.1664e-04 | Grad Norm: 0.1651\n",
      "Epoch 140 | Loss: 0.063733 | LR: 1.1748e-04 | Grad Norm: 0.1680\n",
      "Epoch 141 | Loss: 0.275241 | LR: 1.1832e-04 | Grad Norm: 0.1889\n",
      "Epoch 142 | Loss: 0.220292 | LR: 1.1916e-04 | Grad Norm: 0.1710\n",
      "Epoch 143 | Loss: 0.248154 | LR: 1.2000e-04 | Grad Norm: 0.1815\n",
      "Epoch 144 | Loss: 0.059189 | LR: 1.2084e-04 | Grad Norm: 0.1104\n",
      "Epoch 145 | Loss: 0.218930 | LR: 1.2168e-04 | Grad Norm: 0.1652\n",
      "Epoch 146 | Loss: 0.201620 | LR: 1.2252e-04 | Grad Norm: 0.1568\n",
      "Epoch 147 | Loss: 0.083576 | LR: 1.2336e-04 | Grad Norm: 0.3055\n",
      "Epoch 148 | Loss: 0.130363 | LR: 1.2420e-04 | Grad Norm: 0.1234\n",
      "Epoch 149 | Loss: 0.228040 | LR: 1.2503e-04 | Grad Norm: 0.1852\n",
      "Epoch 150 | Loss: 0.273943 | LR: 1.2587e-04 | Grad Norm: 0.2022\n",
      "Epoch 151 | Loss: 0.270414 | LR: 1.2671e-04 | Grad Norm: 0.1741\n",
      "Epoch 152 | Loss: 0.053620 | LR: 1.2755e-04 | Grad Norm: 0.1096\n",
      "Epoch 153 | Loss: 0.229101 | LR: 1.2839e-04 | Grad Norm: 0.1573\n",
      "Epoch 154 | Loss: 0.182862 | LR: 1.2923e-04 | Grad Norm: 0.1409\n",
      "Epoch 155 | Loss: 0.057734 | LR: 1.3007e-04 | Grad Norm: 0.3318\n",
      "Epoch 156 | Loss: 0.128877 | LR: 1.3091e-04 | Grad Norm: 0.1162\n",
      "Epoch 157 | Loss: 0.177563 | LR: 1.3175e-04 | Grad Norm: 0.1365\n",
      "Epoch 158 | Loss: 0.261537 | LR: 1.3259e-04 | Grad Norm: 0.1870\n",
      "Epoch 159 | Loss: 0.155676 | LR: 1.3343e-04 | Grad Norm: 0.1214\n",
      "Epoch 160 | Loss: 0.151086 | LR: 1.3427e-04 | Grad Norm: 0.1334\n",
      "Epoch 161 | Loss: 0.189319 | LR: 1.3510e-04 | Grad Norm: 0.1659\n",
      "Epoch 162 | Loss: 0.191583 | LR: 1.3594e-04 | Grad Norm: 0.1491\n",
      "Epoch 163 | Loss: 0.221300 | LR: 1.3678e-04 | Grad Norm: 0.1817\n",
      "Epoch 164 | Loss: 0.066973 | LR: 1.3762e-04 | Grad Norm: 0.3898\n",
      "Epoch 165 | Loss: 0.119892 | LR: 1.3846e-04 | Grad Norm: 0.1404\n",
      "Epoch 166 | Loss: 0.125496 | LR: 1.3930e-04 | Grad Norm: 0.1502\n",
      "Epoch 167 | Loss: 0.221360 | LR: 1.4014e-04 | Grad Norm: 0.2939\n",
      "Epoch 168 | Loss: 0.166072 | LR: 1.4098e-04 | Grad Norm: 0.1933\n",
      "Epoch 169 | Loss: 0.166143 | LR: 1.4182e-04 | Grad Norm: 0.1752\n",
      "Epoch 170 | Loss: 0.194964 | LR: 1.4266e-04 | Grad Norm: 0.2035\n",
      "Epoch 171 | Loss: 0.141672 | LR: 1.4350e-04 | Grad Norm: 0.2468\n",
      "Epoch 172 | Loss: 0.234927 | LR: 1.4434e-04 | Grad Norm: 0.2182\n",
      "Epoch 173 | Loss: 0.096042 | LR: 1.4517e-04 | Grad Norm: 0.1347\n",
      "Epoch 174 | Loss: 0.122038 | LR: 1.4601e-04 | Grad Norm: 0.1376\n",
      "Epoch 175 | Loss: 0.145733 | LR: 1.4685e-04 | Grad Norm: 0.1434\n",
      "Epoch 176 | Loss: 0.208724 | LR: 1.4769e-04 | Grad Norm: 0.2016\n",
      "Epoch 177 | Loss: 0.274391 | LR: 1.4853e-04 | Grad Norm: 0.2947\n",
      "Epoch 178 | Loss: 0.186014 | LR: 1.4937e-04 | Grad Norm: 0.1771\n",
      "Epoch 179 | Loss: 0.168594 | LR: 1.5021e-04 | Grad Norm: 0.2026\n",
      "Epoch 180 | Loss: 0.088570 | LR: 1.5105e-04 | Grad Norm: 0.1287\n",
      "Epoch 181 | Loss: 0.145748 | LR: 1.5189e-04 | Grad Norm: 0.1750\n",
      "Epoch 182 | Loss: 0.069511 | LR: 1.5273e-04 | Grad Norm: 0.0906\n",
      "Epoch 183 | Loss: 0.146589 | LR: 1.5357e-04 | Grad Norm: 0.1848\n",
      "Epoch 184 | Loss: 0.065214 | LR: 1.5441e-04 | Grad Norm: 0.2258\n",
      "Epoch 185 | Loss: 0.212127 | LR: 1.5524e-04 | Grad Norm: 0.2817\n",
      "Epoch 186 | Loss: 0.073473 | LR: 1.5608e-04 | Grad Norm: 0.3925\n",
      "Epoch 187 | Loss: 0.185160 | LR: 1.5692e-04 | Grad Norm: 0.1378\n",
      "Epoch 188 | Loss: 0.071296 | LR: 1.5776e-04 | Grad Norm: 0.0725\n",
      "Epoch 189 | Loss: 0.058587 | LR: 1.5860e-04 | Grad Norm: 0.1264\n",
      "Epoch 190 | Loss: 0.074279 | LR: 1.5944e-04 | Grad Norm: 0.0890\n",
      "Epoch 191 | Loss: 0.181909 | LR: 1.6028e-04 | Grad Norm: 0.2937\n",
      "Epoch 192 | Loss: 0.158192 | LR: 1.6112e-04 | Grad Norm: 0.1930\n",
      "Epoch 193 | Loss: 0.164644 | LR: 1.6196e-04 | Grad Norm: 0.1797\n",
      "Epoch 194 | Loss: 0.161352 | LR: 1.6280e-04 | Grad Norm: 0.1729\n",
      "Epoch 195 | Loss: 0.072596 | LR: 1.6364e-04 | Grad Norm: 0.2821\n",
      "Epoch 196 | Loss: 0.078736 | LR: 1.6448e-04 | Grad Norm: 0.0976\n",
      "Epoch 197 | Loss: 0.079915 | LR: 1.6531e-04 | Grad Norm: 0.1219\n",
      "Epoch 198 | Loss: 0.194717 | LR: 1.6615e-04 | Grad Norm: 0.1345\n",
      "Epoch 199 | Loss: 0.058775 | LR: 1.6699e-04 | Grad Norm: 0.0771\n",
      "Epoch 200 | Loss: 0.106768 | LR: 1.6783e-04 | Grad Norm: 0.1125\n",
      "Epoch 201 | Loss: 0.257035 | LR: 1.6867e-04 | Grad Norm: 0.1809\n",
      "Epoch 202 | Loss: 0.169019 | LR: 1.6951e-04 | Grad Norm: 0.1721\n",
      "Epoch 203 | Loss: 0.199718 | LR: 1.7035e-04 | Grad Norm: 0.1480\n",
      "Epoch 204 | Loss: 0.081006 | LR: 1.7119e-04 | Grad Norm: 0.1042\n",
      "Epoch 205 | Loss: 0.207316 | LR: 1.7203e-04 | Grad Norm: 0.1629\n",
      "Epoch 206 | Loss: 0.160239 | LR: 1.7287e-04 | Grad Norm: 0.1621\n",
      "Epoch 207 | Loss: 0.060798 | LR: 1.7371e-04 | Grad Norm: 0.1637\n",
      "Epoch 208 | Loss: 0.117846 | LR: 1.7455e-04 | Grad Norm: 0.9299\n",
      "Epoch 209 | Loss: 0.185550 | LR: 1.7538e-04 | Grad Norm: 0.3010\n",
      "Epoch 210 | Loss: 0.198824 | LR: 1.7622e-04 | Grad Norm: 0.2250\n",
      "Epoch 211 | Loss: 0.212905 | LR: 1.7706e-04 | Grad Norm: 0.1446\n",
      "Epoch 212 | Loss: 0.058731 | LR: 1.7790e-04 | Grad Norm: 0.1126\n",
      "Epoch 213 | Loss: 0.184540 | LR: 1.7874e-04 | Grad Norm: 0.1386\n",
      "Epoch 214 | Loss: 0.160128 | LR: 1.7958e-04 | Grad Norm: 0.1460\n",
      "Epoch 215 | Loss: 0.049068 | LR: 1.8042e-04 | Grad Norm: 0.0809\n",
      "Epoch 216 | Loss: 0.106503 | LR: 1.8126e-04 | Grad Norm: 0.4406\n",
      "Epoch 217 | Loss: 0.146282 | LR: 1.8210e-04 | Grad Norm: 0.4052\n",
      "Epoch 218 | Loss: 0.214140 | LR: 1.8294e-04 | Grad Norm: 0.1873\n",
      "Epoch 219 | Loss: 0.131297 | LR: 1.8378e-04 | Grad Norm: 0.1608\n",
      "Epoch 220 | Loss: 0.144420 | LR: 1.8462e-04 | Grad Norm: 0.1911\n",
      "Epoch 221 | Loss: 0.173487 | LR: 1.8545e-04 | Grad Norm: 0.1404\n",
      "Epoch 222 | Loss: 0.176633 | LR: 1.8629e-04 | Grad Norm: 0.1772\n",
      "Epoch 223 | Loss: 0.179186 | LR: 1.8713e-04 | Grad Norm: 0.2030\n",
      "Epoch 224 | Loss: 0.063940 | LR: 1.8797e-04 | Grad Norm: 0.1980\n",
      "Epoch 225 | Loss: 0.110109 | LR: 1.8881e-04 | Grad Norm: 0.1207\n",
      "Epoch 226 | Loss: 0.113589 | LR: 1.8965e-04 | Grad Norm: 0.1142\n",
      "Epoch 227 | Loss: 0.184983 | LR: 1.9049e-04 | Grad Norm: 0.3599\n",
      "Epoch 228 | Loss: 0.151734 | LR: 1.9133e-04 | Grad Norm: 0.2585\n",
      "Epoch 229 | Loss: 0.133564 | LR: 1.9217e-04 | Grad Norm: 0.2133\n",
      "Epoch 230 | Loss: 0.173698 | LR: 1.9301e-04 | Grad Norm: 0.2853\n",
      "Epoch 231 | Loss: 0.121235 | LR: 1.9385e-04 | Grad Norm: 0.1372\n",
      "Epoch 232 | Loss: 0.181665 | LR: 1.9469e-04 | Grad Norm: 0.1632\n",
      "Epoch 233 | Loss: 0.115157 | LR: 1.9552e-04 | Grad Norm: 0.3673\n",
      "Epoch 234 | Loss: 0.101648 | LR: 1.9636e-04 | Grad Norm: 0.2260\n",
      "Epoch 235 | Loss: 0.152927 | LR: 1.9720e-04 | Grad Norm: 0.1911\n",
      "Epoch 236 | Loss: 0.184125 | LR: 1.9804e-04 | Grad Norm: 0.1613\n",
      "Epoch 237 | Loss: 0.229371 | LR: 1.9888e-04 | Grad Norm: 0.1735\n",
      "Epoch 238 | Loss: 0.148950 | LR: 1.9972e-04 | Grad Norm: 0.2096\n",
      "Epoch 239 | Loss: 0.131123 | LR: 2.0056e-04 | Grad Norm: 0.1213\n",
      "Epoch 240 | Loss: 0.034209 | LR: 2.0140e-04 | Grad Norm: 0.0662\n",
      "Epoch 241 | Loss: 0.038078 | LR: 2.0224e-04 | Grad Norm: 0.0591\n",
      "Epoch 242 | Loss: 0.033991 | LR: 2.0308e-04 | Grad Norm: 0.1591\n",
      "Epoch 243 | Loss: 0.036992 | LR: 2.0392e-04 | Grad Norm: 0.1328\n",
      "Epoch 244 | Loss: 0.032124 | LR: 2.0476e-04 | Grad Norm: 0.1355\n",
      "Epoch 245 | Loss: 0.036862 | LR: 2.0559e-04 | Grad Norm: 0.2221\n",
      "Epoch 246 | Loss: 0.031628 | LR: 2.0643e-04 | Grad Norm: 0.0797\n",
      "Epoch 247 | Loss: 0.124218 | LR: 2.0727e-04 | Grad Norm: 0.4896\n",
      "Epoch 248 | Loss: 0.192844 | LR: 2.0811e-04 | Grad Norm: 0.2029\n",
      "Epoch 249 | Loss: 0.108341 | LR: 2.0895e-04 | Grad Norm: 0.2592\n",
      "Epoch 250 | Loss: 0.221729 | LR: 2.0979e-04 | Grad Norm: 0.3330\n",
      "Epoch 251 | Loss: 0.209072 | LR: 2.1063e-04 | Grad Norm: 0.2232\n",
      "Epoch 252 | Loss: 0.225718 | LR: 2.1147e-04 | Grad Norm: 0.2147\n",
      "Epoch 253 | Loss: 0.186837 | LR: 2.1231e-04 | Grad Norm: 0.2303\n",
      "Epoch 254 | Loss: 0.088830 | LR: 2.1315e-04 | Grad Norm: 0.1503\n",
      "Epoch 255 | Loss: 0.163981 | LR: 2.1399e-04 | Grad Norm: 0.5919\n",
      "Epoch 256 | Loss: 0.210186 | LR: 2.1483e-04 | Grad Norm: 0.2771\n",
      "Epoch 257 | Loss: 0.191867 | LR: 2.1566e-04 | Grad Norm: 0.2278\n",
      "Epoch 258 | Loss: 0.427602 | LR: 2.1650e-04 | Grad Norm: 0.4357\n",
      "Epoch 259 | Loss: 0.140471 | LR: 2.1734e-04 | Grad Norm: 0.2456\n",
      "Epoch 260 | Loss: 0.277596 | LR: 2.1818e-04 | Grad Norm: 0.2513\n",
      "Epoch 261 | Loss: 0.250203 | LR: 2.1902e-04 | Grad Norm: 0.2828\n",
      "Epoch 262 | Loss: 0.036348 | LR: 2.1986e-04 | Grad Norm: 0.1112\n",
      "Epoch 263 | Loss: 0.134995 | LR: 2.2070e-04 | Grad Norm: 0.2015\n",
      "Epoch 264 | Loss: 0.235224 | LR: 2.2154e-04 | Grad Norm: 0.7883\n",
      "Epoch 265 | Loss: 0.211808 | LR: 2.2238e-04 | Grad Norm: 0.2482\n",
      "Epoch 266 | Loss: 0.034401 | LR: 2.2322e-04 | Grad Norm: 0.0575\n",
      "Epoch 267 | Loss: 0.036920 | LR: 2.2406e-04 | Grad Norm: 0.5841\n",
      "Epoch 268 | Loss: 0.034401 | LR: 2.2490e-04 | Grad Norm: 0.0675\n",
      "Epoch 269 | Loss: 0.030857 | LR: 2.2573e-04 | Grad Norm: 0.2145\n",
      "Epoch 270 | Loss: 0.037876 | LR: 2.2657e-04 | Grad Norm: 0.1114\n",
      "Epoch 271 | Loss: 0.109410 | LR: 2.2741e-04 | Grad Norm: 0.1416\n",
      "Epoch 272 | Loss: 0.144882 | LR: 2.2825e-04 | Grad Norm: 0.1938\n",
      "Epoch 273 | Loss: 0.119881 | LR: 2.2909e-04 | Grad Norm: 0.5256\n",
      "Epoch 274 | Loss: 0.152448 | LR: 2.2993e-04 | Grad Norm: 0.1654\n",
      "Epoch 275 | Loss: 0.103801 | LR: 2.3077e-04 | Grad Norm: 0.2013\n",
      "Epoch 276 | Loss: 0.029647 | LR: 2.3161e-04 | Grad Norm: 0.0880\n",
      "Epoch 277 | Loss: 0.033847 | LR: 2.3245e-04 | Grad Norm: 0.1228\n",
      "Epoch 278 | Loss: 0.154281 | LR: 2.3329e-04 | Grad Norm: 0.2467\n",
      "Epoch 279 | Loss: 0.105547 | LR: 2.3413e-04 | Grad Norm: 0.1216\n",
      "Epoch 280 | Loss: 0.180816 | LR: 2.3497e-04 | Grad Norm: 0.4092\n",
      "Epoch 281 | Loss: 0.174674 | LR: 2.3580e-04 | Grad Norm: 0.2109\n",
      "Epoch 282 | Loss: 0.199266 | LR: 2.3664e-04 | Grad Norm: 0.5703\n",
      "Epoch 283 | Loss: 0.122369 | LR: 2.3748e-04 | Grad Norm: 0.1607\n",
      "Epoch 284 | Loss: 0.130384 | LR: 2.3832e-04 | Grad Norm: 0.2217\n",
      "Epoch 285 | Loss: 0.124554 | LR: 2.3916e-04 | Grad Norm: 0.1922\n",
      "Epoch 286 | Loss: 0.125715 | LR: 2.4000e-04 | Grad Norm: 0.1833\n",
      "Epoch 287 | Loss: 0.161766 | LR: 2.4084e-04 | Grad Norm: 0.3982\n",
      "Epoch 288 | Loss: 0.148818 | LR: 2.4168e-04 | Grad Norm: 0.1866\n",
      "Epoch 289 | Loss: 0.033399 | LR: 2.4252e-04 | Grad Norm: 0.1842\n",
      "Epoch 290 | Loss: 0.142109 | LR: 2.4336e-04 | Grad Norm: 0.7686\n",
      "Epoch 291 | Loss: 0.103473 | LR: 2.4420e-04 | Grad Norm: 0.1809\n",
      "Epoch 292 | Loss: 0.033404 | LR: 2.4503e-04 | Grad Norm: 0.2837\n",
      "Epoch 293 | Loss: 0.171316 | LR: 2.4587e-04 | Grad Norm: 0.2847\n",
      "Epoch 294 | Loss: 0.031600 | LR: 2.4671e-04 | Grad Norm: 0.0914\n",
      "Epoch 295 | Loss: 0.199890 | LR: 2.4755e-04 | Grad Norm: 0.4636\n",
      "Epoch 296 | Loss: 0.171385 | LR: 2.4839e-04 | Grad Norm: 0.3152\n",
      "Epoch 297 | Loss: 0.176337 | LR: 2.4923e-04 | Grad Norm: 0.4524\n",
      "Epoch 298 | Loss: 0.029932 | LR: 2.5007e-04 | Grad Norm: 0.0887\n",
      "Epoch 299 | Loss: 0.033243 | LR: 2.5091e-04 | Grad Norm: 0.1476\n",
      "Epoch 300 | Loss: 0.048179 | LR: 2.5175e-04 | Grad Norm: 0.0989\n",
      "Epoch 301 | Loss: 0.040580 | LR: 2.5259e-04 | Grad Norm: 0.1302\n",
      "Epoch 302 | Loss: 0.047772 | LR: 2.5343e-04 | Grad Norm: 0.0932\n",
      "Epoch 303 | Loss: 0.038237 | LR: 2.5427e-04 | Grad Norm: 0.1548\n",
      "Epoch 304 | Loss: 0.082815 | LR: 2.5510e-04 | Grad Norm: 0.2750\n",
      "Epoch 305 | Loss: 0.035861 | LR: 2.5594e-04 | Grad Norm: 0.1194\n",
      "Epoch 306 | Loss: 0.064899 | LR: 2.5678e-04 | Grad Norm: 0.1580\n",
      "Epoch 307 | Loss: 0.109343 | LR: 2.5762e-04 | Grad Norm: 0.1345\n",
      "Epoch 308 | Loss: 0.160416 | LR: 2.5846e-04 | Grad Norm: 0.2437\n",
      "Epoch 309 | Loss: 0.097077 | LR: 2.5930e-04 | Grad Norm: 0.1297\n",
      "Epoch 310 | Loss: 0.188428 | LR: 2.6014e-04 | Grad Norm: 0.1641\n",
      "Epoch 311 | Loss: 0.177466 | LR: 2.6098e-04 | Grad Norm: 0.1544\n",
      "Epoch 312 | Loss: 0.194444 | LR: 2.6182e-04 | Grad Norm: 0.1954\n",
      "Epoch 313 | Loss: 0.175069 | LR: 2.6266e-04 | Grad Norm: 0.2676\n",
      "Epoch 314 | Loss: 0.079748 | LR: 2.6350e-04 | Grad Norm: 0.1622\n",
      "Epoch 315 | Loss: 0.129534 | LR: 2.6434e-04 | Grad Norm: 0.3032\n",
      "Epoch 316 | Loss: 0.166735 | LR: 2.6517e-04 | Grad Norm: 0.1962\n",
      "Epoch 317 | Loss: 0.150584 | LR: 2.6601e-04 | Grad Norm: 0.1699\n",
      "Epoch 318 | Loss: 0.272534 | LR: 2.6685e-04 | Grad Norm: 0.3263\n",
      "Epoch 319 | Loss: 0.115884 | LR: 2.6769e-04 | Grad Norm: 0.3081\n",
      "Epoch 320 | Loss: 0.257826 | LR: 2.6853e-04 | Grad Norm: 0.2203\n",
      "Epoch 321 | Loss: 0.214565 | LR: 2.6937e-04 | Grad Norm: 0.2418\n",
      "Epoch 322 | Loss: 0.071072 | LR: 2.7021e-04 | Grad Norm: 0.1271\n",
      "Epoch 323 | Loss: 0.118981 | LR: 2.7105e-04 | Grad Norm: 0.2315\n",
      "Epoch 324 | Loss: 0.211977 | LR: 2.7189e-04 | Grad Norm: 0.1608\n",
      "Epoch 325 | Loss: 0.212411 | LR: 2.7273e-04 | Grad Norm: 0.4993\n",
      "Epoch 326 | Loss: 0.037798 | LR: 2.7357e-04 | Grad Norm: 0.1209\n",
      "Epoch 327 | Loss: 0.038397 | LR: 2.7441e-04 | Grad Norm: 0.1029\n",
      "Epoch 328 | Loss: 0.064771 | LR: 2.7524e-04 | Grad Norm: 0.0896\n",
      "Epoch 329 | Loss: 0.071169 | LR: 2.7608e-04 | Grad Norm: 0.1488\n",
      "Epoch 330 | Loss: 0.084526 | LR: 2.7692e-04 | Grad Norm: 0.1659\n",
      "Epoch 331 | Loss: 0.090999 | LR: 2.7776e-04 | Grad Norm: 0.1054\n",
      "Epoch 332 | Loss: 0.150478 | LR: 2.7860e-04 | Grad Norm: 0.1620\n",
      "Epoch 333 | Loss: 0.113946 | LR: 2.7944e-04 | Grad Norm: 0.2245\n",
      "Epoch 334 | Loss: 0.122898 | LR: 2.8028e-04 | Grad Norm: 0.1557\n",
      "Epoch 335 | Loss: 0.100647 | LR: 2.8112e-04 | Grad Norm: 0.2173\n",
      "Epoch 336 | Loss: 0.049710 | LR: 2.8196e-04 | Grad Norm: 0.1915\n",
      "Epoch 337 | Loss: 0.082730 | LR: 2.8280e-04 | Grad Norm: 0.2498\n",
      "Epoch 338 | Loss: 0.139171 | LR: 2.8364e-04 | Grad Norm: 0.1597\n",
      "Epoch 339 | Loss: 0.104550 | LR: 2.8448e-04 | Grad Norm: 0.1224\n",
      "Epoch 340 | Loss: 0.164012 | LR: 2.8531e-04 | Grad Norm: 0.2533\n",
      "Epoch 341 | Loss: 0.167101 | LR: 2.8615e-04 | Grad Norm: 0.3325\n",
      "Epoch 342 | Loss: 0.190520 | LR: 2.8699e-04 | Grad Norm: 0.3835\n",
      "Epoch 343 | Loss: 0.102355 | LR: 2.8783e-04 | Grad Norm: 0.2278\n",
      "Epoch 344 | Loss: 0.116158 | LR: 2.8867e-04 | Grad Norm: 0.1778\n",
      "Epoch 345 | Loss: 0.115379 | LR: 2.8951e-04 | Grad Norm: 0.1204\n",
      "Epoch 346 | Loss: 0.136285 | LR: 2.9035e-04 | Grad Norm: 0.1350\n",
      "Epoch 347 | Loss: 0.147883 | LR: 2.9119e-04 | Grad Norm: 0.2803\n",
      "Epoch 348 | Loss: 0.145773 | LR: 2.9203e-04 | Grad Norm: 0.1486\n",
      "Epoch 349 | Loss: 0.059441 | LR: 2.9287e-04 | Grad Norm: 0.0883\n",
      "Epoch 350 | Loss: 0.122830 | LR: 2.9371e-04 | Grad Norm: 0.1373\n",
      "Epoch 351 | Loss: 0.133144 | LR: 2.9455e-04 | Grad Norm: 0.2537\n",
      "Epoch 352 | Loss: 0.034472 | LR: 2.9538e-04 | Grad Norm: 0.1120\n",
      "Epoch 353 | Loss: 0.143210 | LR: 2.9622e-04 | Grad Norm: 0.1442\n",
      "Epoch 354 | Loss: 0.046423 | LR: 2.9706e-04 | Grad Norm: 0.1962\n",
      "Epoch 355 | Loss: 0.175473 | LR: 2.9790e-04 | Grad Norm: 0.2341\n",
      "Epoch 356 | Loss: 0.181595 | LR: 2.9874e-04 | Grad Norm: 0.2163\n",
      "Epoch 357 | Loss: 0.154244 | LR: 2.9958e-04 | Grad Norm: 0.2368\n",
      "Epoch 358 | Loss: 0.047826 | LR: 3.0042e-04 | Grad Norm: 0.1318\n",
      "Epoch 359 | Loss: 0.040662 | LR: 3.0126e-04 | Grad Norm: 0.1381\n",
      "Epoch 360 | Loss: 0.161925 | LR: 3.0210e-04 | Grad Norm: 0.3692\n",
      "Epoch 361 | Loss: 0.070725 | LR: 3.0294e-04 | Grad Norm: 0.3373\n",
      "Epoch 362 | Loss: 0.160067 | LR: 3.0378e-04 | Grad Norm: 0.2492\n",
      "Epoch 363 | Loss: 0.062045 | LR: 3.0462e-04 | Grad Norm: 0.1495\n",
      "Epoch 364 | Loss: 0.230546 | LR: 3.0545e-04 | Grad Norm: 0.5356\n",
      "Epoch 365 | Loss: 0.062655 | LR: 3.0629e-04 | Grad Norm: 0.7628\n",
      "Epoch 366 | Loss: 0.186756 | LR: 3.0713e-04 | Grad Norm: 0.1990\n",
      "Epoch 367 | Loss: 0.032810 | LR: 3.0797e-04 | Grad Norm: 0.2428\n",
      "Epoch 368 | Loss: 0.044502 | LR: 3.0881e-04 | Grad Norm: 0.3075\n",
      "Epoch 369 | Loss: 0.024630 | LR: 3.0965e-04 | Grad Norm: 0.1230\n",
      "Epoch 370 | Loss: 0.156763 | LR: 3.1049e-04 | Grad Norm: 0.1657\n",
      "Epoch 371 | Loss: 0.118320 | LR: 3.1133e-04 | Grad Norm: 0.1798\n",
      "Epoch 372 | Loss: 0.140183 | LR: 3.1217e-04 | Grad Norm: 0.2208\n",
      "Epoch 373 | Loss: 0.172937 | LR: 3.1301e-04 | Grad Norm: 0.2814\n",
      "Epoch 374 | Loss: 0.032551 | LR: 3.1385e-04 | Grad Norm: 0.0927\n",
      "Epoch 375 | Loss: 0.045501 | LR: 3.1469e-04 | Grad Norm: 0.6472\n",
      "Epoch 376 | Loss: 0.032919 | LR: 3.1552e-04 | Grad Norm: 0.0731\n",
      "Epoch 377 | Loss: 0.027992 | LR: 3.1636e-04 | Grad Norm: 0.0719\n",
      "Epoch 378 | Loss: 0.033603 | LR: 3.1720e-04 | Grad Norm: 0.1363\n",
      "Epoch 379 | Loss: 0.026998 | LR: 3.1804e-04 | Grad Norm: 0.1322\n",
      "Epoch 380 | Loss: 0.239985 | LR: 3.1888e-04 | Grad Norm: 0.5266\n",
      "Epoch 381 | Loss: 0.187305 | LR: 3.1972e-04 | Grad Norm: 0.2649\n",
      "Epoch 382 | Loss: 0.215804 | LR: 3.2056e-04 | Grad Norm: 0.1887\n",
      "Epoch 383 | Loss: 0.028284 | LR: 3.2140e-04 | Grad Norm: 0.1439\n",
      "Epoch 384 | Loss: 0.176510 | LR: 3.2224e-04 | Grad Norm: 0.4331\n",
      "Epoch 385 | Loss: 0.172203 | LR: 3.2308e-04 | Grad Norm: 0.9530\n",
      "Epoch 386 | Loss: 0.080356 | LR: 3.2392e-04 | Grad Norm: 0.2094\n",
      "Epoch 387 | Loss: 0.118196 | LR: 3.2476e-04 | Grad Norm: 0.1730\n",
      "Epoch 388 | Loss: 0.213930 | LR: 3.2559e-04 | Grad Norm: 0.2502\n",
      "Epoch 389 | Loss: 0.222738 | LR: 3.2643e-04 | Grad Norm: 0.1883\n",
      "Epoch 390 | Loss: 0.225312 | LR: 3.2727e-04 | Grad Norm: 0.2166\n",
      "Epoch 391 | Loss: 0.023058 | LR: 3.2811e-04 | Grad Norm: 0.0550\n",
      "Epoch 392 | Loss: 0.173658 | LR: 3.2895e-04 | Grad Norm: 0.1815\n",
      "Epoch 393 | Loss: 0.137126 | LR: 3.2979e-04 | Grad Norm: 0.2839\n",
      "Epoch 394 | Loss: 0.032168 | LR: 3.3063e-04 | Grad Norm: 0.3172\n",
      "Epoch 395 | Loss: 0.097193 | LR: 3.3147e-04 | Grad Norm: 0.2558\n",
      "Epoch 396 | Loss: 0.144077 | LR: 3.3231e-04 | Grad Norm: 1.0134\n",
      "Epoch 397 | Loss: 0.231363 | LR: 3.3315e-04 | Grad Norm: 0.4276\n",
      "Epoch 398 | Loss: 0.118525 | LR: 3.3399e-04 | Grad Norm: 0.1998\n",
      "Epoch 399 | Loss: 0.113425 | LR: 3.3483e-04 | Grad Norm: 0.1871\n",
      "Epoch 400 | Loss: 0.158673 | LR: 3.3566e-04 | Grad Norm: 0.6581\n",
      "Epoch 401 | Loss: 0.149099 | LR: 3.3650e-04 | Grad Norm: 0.3482\n",
      "Epoch 402 | Loss: 0.172696 | LR: 3.3734e-04 | Grad Norm: 0.3288\n",
      "Epoch 403 | Loss: 0.025589 | LR: 3.3818e-04 | Grad Norm: 0.1202\n",
      "Epoch 404 | Loss: 0.100459 | LR: 3.3902e-04 | Grad Norm: 0.3715\n",
      "Epoch 405 | Loss: 0.099218 | LR: 3.3986e-04 | Grad Norm: 0.1276\n",
      "Epoch 406 | Loss: 0.178375 | LR: 3.4070e-04 | Grad Norm: 0.2642\n",
      "Epoch 407 | Loss: 0.132075 | LR: 3.4154e-04 | Grad Norm: 0.2267\n",
      "Epoch 408 | Loss: 0.142607 | LR: 3.4238e-04 | Grad Norm: 0.4163\n",
      "Epoch 409 | Loss: 0.171388 | LR: 3.4322e-04 | Grad Norm: 0.2276\n",
      "Epoch 410 | Loss: 0.109470 | LR: 3.4406e-04 | Grad Norm: 0.1822\n",
      "Epoch 411 | Loss: 0.189884 | LR: 3.4490e-04 | Grad Norm: 0.3941\n",
      "Epoch 412 | Loss: 0.094768 | LR: 3.4573e-04 | Grad Norm: 0.6651\n",
      "Epoch 413 | Loss: 0.103834 | LR: 3.4657e-04 | Grad Norm: 0.9020\n",
      "Epoch 414 | Loss: 0.111516 | LR: 3.4741e-04 | Grad Norm: 0.1652\n",
      "Epoch 415 | Loss: 0.157872 | LR: 3.4825e-04 | Grad Norm: 0.4826\n",
      "Epoch 416 | Loss: 0.208128 | LR: 3.4909e-04 | Grad Norm: 0.2119\n",
      "Epoch 417 | Loss: 0.153147 | LR: 3.4993e-04 | Grad Norm: 0.1529\n",
      "Epoch 418 | Loss: 0.138939 | LR: 3.5077e-04 | Grad Norm: 0.1577\n",
      "Epoch 419 | Loss: 0.080877 | LR: 3.5161e-04 | Grad Norm: 0.1364\n",
      "Epoch 420 | Loss: 0.117001 | LR: 3.5245e-04 | Grad Norm: 0.1496\n",
      "Epoch 421 | Loss: 0.058206 | LR: 3.5329e-04 | Grad Norm: 0.2043\n",
      "Epoch 422 | Loss: 0.122258 | LR: 3.5413e-04 | Grad Norm: 0.1738\n",
      "Epoch 423 | Loss: 0.053146 | LR: 3.5497e-04 | Grad Norm: 0.1727\n",
      "Epoch 424 | Loss: 0.171477 | LR: 3.5580e-04 | Grad Norm: 0.5368\n",
      "Epoch 425 | Loss: 0.053837 | LR: 3.5664e-04 | Grad Norm: 0.3951\n",
      "Epoch 426 | Loss: 0.164384 | LR: 3.5748e-04 | Grad Norm: 0.7889\n",
      "Epoch 427 | Loss: 0.058107 | LR: 3.5832e-04 | Grad Norm: 0.1359\n",
      "Epoch 428 | Loss: 0.041432 | LR: 3.5916e-04 | Grad Norm: 0.1078\n",
      "Epoch 429 | Loss: 0.054122 | LR: 3.6000e-04 | Grad Norm: 0.0724\n",
      "Epoch 430 | Loss: 0.156517 | LR: 3.6084e-04 | Grad Norm: 0.1245\n",
      "Epoch 431 | Loss: 0.133032 | LR: 3.6168e-04 | Grad Norm: 0.1797\n",
      "Epoch 432 | Loss: 0.138061 | LR: 3.6252e-04 | Grad Norm: 0.1744\n",
      "Epoch 433 | Loss: 0.140429 | LR: 3.6336e-04 | Grad Norm: 0.1462\n",
      "Epoch 434 | Loss: 0.052117 | LR: 3.6420e-04 | Grad Norm: 0.1643\n",
      "Epoch 435 | Loss: 0.073172 | LR: 3.6503e-04 | Grad Norm: 0.2121\n",
      "Epoch 436 | Loss: 0.058600 | LR: 3.6587e-04 | Grad Norm: 0.1314\n",
      "Epoch 437 | Loss: 0.163499 | LR: 3.6671e-04 | Grad Norm: 0.2479\n",
      "Epoch 438 | Loss: 0.042279 | LR: 3.6755e-04 | Grad Norm: 0.0830\n",
      "Epoch 439 | Loss: 0.088280 | LR: 3.6839e-04 | Grad Norm: 0.1388\n",
      "Epoch 440 | Loss: 0.214676 | LR: 3.6923e-04 | Grad Norm: 0.1837\n",
      "Epoch 441 | Loss: 0.148668 | LR: 3.7007e-04 | Grad Norm: 0.3676\n",
      "Epoch 442 | Loss: 0.183712 | LR: 3.7091e-04 | Grad Norm: 0.1890\n",
      "Epoch 443 | Loss: 0.061285 | LR: 3.7175e-04 | Grad Norm: 0.1283\n",
      "Epoch 444 | Loss: 0.171695 | LR: 3.7259e-04 | Grad Norm: 0.1997\n",
      "Epoch 445 | Loss: 0.132766 | LR: 3.7343e-04 | Grad Norm: 0.2159\n",
      "Epoch 446 | Loss: 0.076841 | LR: 3.7427e-04 | Grad Norm: 0.1756\n",
      "Epoch 447 | Loss: 0.096506 | LR: 3.7510e-04 | Grad Norm: 0.2111\n",
      "Epoch 448 | Loss: 0.165955 | LR: 3.7594e-04 | Grad Norm: 0.1932\n",
      "Epoch 449 | Loss: 0.170614 | LR: 3.7678e-04 | Grad Norm: 0.1440\n",
      "Epoch 450 | Loss: 0.181206 | LR: 3.7762e-04 | Grad Norm: 0.1894\n",
      "Epoch 451 | Loss: 0.040673 | LR: 3.7846e-04 | Grad Norm: 0.1300\n",
      "Epoch 452 | Loss: 0.145348 | LR: 3.7930e-04 | Grad Norm: 0.1301\n",
      "Epoch 453 | Loss: 0.130255 | LR: 3.8014e-04 | Grad Norm: 0.2500\n",
      "Epoch 454 | Loss: 0.041138 | LR: 3.8098e-04 | Grad Norm: 0.1610\n",
      "Epoch 455 | Loss: 0.082714 | LR: 3.8182e-04 | Grad Norm: 0.1691\n",
      "Epoch 456 | Loss: 0.112438 | LR: 3.8266e-04 | Grad Norm: 0.1282\n",
      "Epoch 457 | Loss: 0.197863 | LR: 3.8350e-04 | Grad Norm: 0.4317\n",
      "Epoch 458 | Loss: 0.107692 | LR: 3.8434e-04 | Grad Norm: 0.1641\n",
      "Epoch 459 | Loss: 0.121151 | LR: 3.8517e-04 | Grad Norm: 0.3652\n",
      "Epoch 460 | Loss: 0.151413 | LR: 3.8601e-04 | Grad Norm: 0.2090\n",
      "Epoch 461 | Loss: 0.145286 | LR: 3.8685e-04 | Grad Norm: 0.1428\n",
      "Epoch 462 | Loss: 0.150714 | LR: 3.8769e-04 | Grad Norm: 0.1391\n",
      "Epoch 463 | Loss: 0.035854 | LR: 3.8853e-04 | Grad Norm: 0.0718\n",
      "Epoch 464 | Loss: 0.098150 | LR: 3.8937e-04 | Grad Norm: 0.1253\n",
      "Epoch 465 | Loss: 0.099407 | LR: 3.9021e-04 | Grad Norm: 0.1473\n",
      "Epoch 466 | Loss: 0.155641 | LR: 3.9105e-04 | Grad Norm: 0.1651\n",
      "Epoch 467 | Loss: 0.128908 | LR: 3.9189e-04 | Grad Norm: 0.2565\n",
      "Epoch 468 | Loss: 0.121372 | LR: 3.9273e-04 | Grad Norm: 0.4656\n",
      "Epoch 469 | Loss: 0.151628 | LR: 3.9357e-04 | Grad Norm: 0.1884\n",
      "Epoch 470 | Loss: 0.102497 | LR: 3.9441e-04 | Grad Norm: 0.1700\n",
      "Epoch 471 | Loss: 0.145095 | LR: 3.9524e-04 | Grad Norm: 0.1647\n",
      "Epoch 472 | Loss: 0.097314 | LR: 3.9608e-04 | Grad Norm: 0.2842\n",
      "Epoch 473 | Loss: 0.083169 | LR: 3.9692e-04 | Grad Norm: 0.2098\n",
      "Epoch 474 | Loss: 0.127947 | LR: 3.9776e-04 | Grad Norm: 0.1594\n",
      "Epoch 475 | Loss: 0.153796 | LR: 3.9860e-04 | Grad Norm: 0.3346\n",
      "Epoch 476 | Loss: 0.181223 | LR: 3.9944e-04 | Grad Norm: 0.1597\n",
      "Epoch 477 | Loss: 0.127273 | LR: 4.0028e-04 | Grad Norm: 0.1805\n",
      "Epoch 478 | Loss: 0.127823 | LR: 4.0112e-04 | Grad Norm: 0.1717\n",
      "Epoch 479 | Loss: 0.021578 | LR: 4.0196e-04 | Grad Norm: 0.1112\n",
      "Epoch 480 | Loss: 0.034091 | LR: 4.0280e-04 | Grad Norm: 0.0698\n",
      "Epoch 481 | Loss: 0.021335 | LR: 4.0364e-04 | Grad Norm: 0.1798\n",
      "Epoch 482 | Loss: 0.029199 | LR: 4.0448e-04 | Grad Norm: 0.1347\n",
      "Epoch 483 | Loss: 0.018548 | LR: 4.0531e-04 | Grad Norm: 0.0672\n",
      "Epoch 484 | Loss: 0.027625 | LR: 4.0615e-04 | Grad Norm: 0.2437\n",
      "Epoch 485 | Loss: 0.022305 | LR: 4.0699e-04 | Grad Norm: 0.0907\n",
      "Epoch 486 | Loss: 0.108295 | LR: 4.0783e-04 | Grad Norm: 0.3393\n",
      "Epoch 487 | Loss: 0.171717 | LR: 4.0867e-04 | Grad Norm: 0.2742\n",
      "Epoch 488 | Loss: 0.091422 | LR: 4.0951e-04 | Grad Norm: 0.1953\n",
      "Epoch 489 | Loss: 0.183871 | LR: 4.1035e-04 | Grad Norm: 0.3008\n",
      "Epoch 490 | Loss: 0.189604 | LR: 4.1119e-04 | Grad Norm: 0.2622\n",
      "Epoch 491 | Loss: 0.195882 | LR: 4.1203e-04 | Grad Norm: 0.1984\n",
      "Epoch 492 | Loss: 0.174244 | LR: 4.1287e-04 | Grad Norm: 0.1906\n",
      "Epoch 493 | Loss: 0.078231 | LR: 4.1371e-04 | Grad Norm: 0.1805\n",
      "Epoch 494 | Loss: 0.142437 | LR: 4.1455e-04 | Grad Norm: 0.2283\n",
      "Epoch 495 | Loss: 0.202503 | LR: 4.1538e-04 | Grad Norm: 0.3243\n",
      "Epoch 496 | Loss: 0.167954 | LR: 4.1622e-04 | Grad Norm: 0.1940\n",
      "Epoch 497 | Loss: 0.369880 | LR: 4.1706e-04 | Grad Norm: 0.6194\n",
      "Epoch 498 | Loss: 0.116845 | LR: 4.1790e-04 | Grad Norm: 0.1538\n",
      "Epoch 499 | Loss: 0.255107 | LR: 4.1874e-04 | Grad Norm: 0.2803\n",
      "Epoch 500 | Loss: 0.211197 | LR: 4.1958e-04 | Grad Norm: 0.3239\n",
      "Epoch 501 | Loss: 0.024084 | LR: 4.2042e-04 | Grad Norm: 0.1313\n",
      "Epoch 502 | Loss: 0.118526 | LR: 4.2126e-04 | Grad Norm: 0.2160\n",
      "Epoch 503 | Loss: 0.196666 | LR: 4.2210e-04 | Grad Norm: 0.2361\n",
      "Epoch 504 | Loss: 0.195242 | LR: 4.2294e-04 | Grad Norm: 0.4208\n",
      "Epoch 505 | Loss: 0.034965 | LR: 4.2378e-04 | Grad Norm: 0.1781\n",
      "Epoch 506 | Loss: 0.019241 | LR: 4.2462e-04 | Grad Norm: 0.3411\n",
      "Epoch 507 | Loss: 0.027005 | LR: 4.2545e-04 | Grad Norm: 0.2605\n",
      "Epoch 508 | Loss: 0.018173 | LR: 4.2629e-04 | Grad Norm: 0.1207\n",
      "Epoch 509 | Loss: 0.038062 | LR: 4.2713e-04 | Grad Norm: 0.2490\n",
      "Epoch 510 | Loss: 0.097444 | LR: 4.2797e-04 | Grad Norm: 0.1201\n",
      "Epoch 511 | Loss: 0.136279 | LR: 4.2881e-04 | Grad Norm: 0.2221\n",
      "Epoch 512 | Loss: 0.098342 | LR: 4.2965e-04 | Grad Norm: 0.1242\n",
      "Epoch 513 | Loss: 0.137584 | LR: 4.3049e-04 | Grad Norm: 0.2334\n",
      "Epoch 514 | Loss: 0.092178 | LR: 4.3133e-04 | Grad Norm: 0.1145\n",
      "Epoch 515 | Loss: 0.020189 | LR: 4.3217e-04 | Grad Norm: 0.1790\n",
      "Epoch 516 | Loss: 0.026491 | LR: 4.3301e-04 | Grad Norm: 0.1042\n",
      "Epoch 517 | Loss: 0.135378 | LR: 4.3385e-04 | Grad Norm: 0.3517\n",
      "Epoch 518 | Loss: 0.097196 | LR: 4.3469e-04 | Grad Norm: 0.6837\n",
      "Epoch 519 | Loss: 0.149965 | LR: 4.3552e-04 | Grad Norm: 0.2241\n",
      "Epoch 520 | Loss: 0.172860 | LR: 4.3636e-04 | Grad Norm: 0.1941\n",
      "Epoch 521 | Loss: 0.172286 | LR: 4.3720e-04 | Grad Norm: 0.7032\n",
      "Epoch 522 | Loss: 0.108452 | LR: 4.3804e-04 | Grad Norm: 0.1843\n",
      "Epoch 523 | Loss: 0.114975 | LR: 4.3888e-04 | Grad Norm: 0.1495\n",
      "Epoch 524 | Loss: 0.116714 | LR: 4.3972e-04 | Grad Norm: 0.3098\n",
      "Epoch 525 | Loss: 0.104229 | LR: 4.4056e-04 | Grad Norm: 0.2286\n",
      "Epoch 526 | Loss: 0.146431 | LR: 4.4140e-04 | Grad Norm: 0.5135\n",
      "Epoch 527 | Loss: 0.146131 | LR: 4.4224e-04 | Grad Norm: 0.2480\n",
      "Epoch 528 | Loss: 0.028158 | LR: 4.4308e-04 | Grad Norm: 0.2349\n",
      "Epoch 529 | Loss: 0.115303 | LR: 4.4392e-04 | Grad Norm: 0.1659\n",
      "Epoch 530 | Loss: 0.096369 | LR: 4.4476e-04 | Grad Norm: 0.2343\n",
      "Epoch 531 | Loss: 0.019229 | LR: 4.4559e-04 | Grad Norm: 0.1633\n",
      "Epoch 532 | Loss: 0.146018 | LR: 4.4643e-04 | Grad Norm: 0.1758\n",
      "Epoch 533 | Loss: 0.022774 | LR: 4.4727e-04 | Grad Norm: 0.1313\n",
      "Epoch 534 | Loss: 0.161563 | LR: 4.4811e-04 | Grad Norm: 0.4226\n",
      "Epoch 535 | Loss: 0.159040 | LR: 4.4895e-04 | Grad Norm: 0.3453\n",
      "Epoch 536 | Loss: 0.158964 | LR: 4.4979e-04 | Grad Norm: 0.4396\n",
      "Epoch 537 | Loss: 0.020180 | LR: 4.5063e-04 | Grad Norm: 0.0782\n",
      "Epoch 538 | Loss: 0.028715 | LR: 4.5147e-04 | Grad Norm: 0.0962\n",
      "Epoch 539 | Loss: 0.035222 | LR: 4.5231e-04 | Grad Norm: 0.0657\n",
      "Epoch 540 | Loss: 0.036825 | LR: 4.5315e-04 | Grad Norm: 0.1117\n",
      "Epoch 541 | Loss: 0.036071 | LR: 4.5399e-04 | Grad Norm: 0.0991\n",
      "Epoch 542 | Loss: 0.033154 | LR: 4.5483e-04 | Grad Norm: 0.3482\n",
      "Epoch 543 | Loss: 0.062198 | LR: 4.5566e-04 | Grad Norm: 0.0986\n",
      "Epoch 544 | Loss: 0.027918 | LR: 4.5650e-04 | Grad Norm: 0.1687\n",
      "Epoch 545 | Loss: 0.047924 | LR: 4.5734e-04 | Grad Norm: 0.2762\n",
      "Epoch 546 | Loss: 0.099666 | LR: 4.5818e-04 | Grad Norm: 0.3730\n",
      "Epoch 547 | Loss: 0.142251 | LR: 4.5902e-04 | Grad Norm: 0.4152\n",
      "Epoch 548 | Loss: 0.083088 | LR: 4.5986e-04 | Grad Norm: 0.1877\n",
      "Epoch 549 | Loss: 0.166067 | LR: 4.6070e-04 | Grad Norm: 0.1625\n",
      "Epoch 550 | Loss: 0.155508 | LR: 4.6154e-04 | Grad Norm: 0.1742\n",
      "Epoch 551 | Loss: 0.166008 | LR: 4.6238e-04 | Grad Norm: 0.1666\n",
      "Epoch 552 | Loss: 0.165103 | LR: 4.6322e-04 | Grad Norm: 0.1832\n",
      "Epoch 553 | Loss: 0.067314 | LR: 4.6406e-04 | Grad Norm: 0.1223\n",
      "Epoch 554 | Loss: 0.111679 | LR: 4.6490e-04 | Grad Norm: 0.1614\n",
      "Epoch 555 | Loss: 0.150590 | LR: 4.6573e-04 | Grad Norm: 0.2047\n",
      "Epoch 556 | Loss: 0.131128 | LR: 4.6657e-04 | Grad Norm: 0.1532\n",
      "Epoch 557 | Loss: 0.240849 | LR: 4.6741e-04 | Grad Norm: 0.2592\n",
      "Epoch 558 | Loss: 0.101188 | LR: 4.6825e-04 | Grad Norm: 0.1379\n",
      "Epoch 559 | Loss: 0.227339 | LR: 4.6909e-04 | Grad Norm: 0.2194\n",
      "Epoch 560 | Loss: 0.184374 | LR: 4.6993e-04 | Grad Norm: 0.2661\n",
      "Epoch 561 | Loss: 0.056107 | LR: 4.7077e-04 | Grad Norm: 0.0994\n",
      "Epoch 562 | Loss: 0.093710 | LR: 4.7161e-04 | Grad Norm: 0.1467\n",
      "Epoch 563 | Loss: 0.180024 | LR: 4.7245e-04 | Grad Norm: 0.1745\n",
      "Epoch 564 | Loss: 0.186540 | LR: 4.7329e-04 | Grad Norm: 0.5422\n",
      "Epoch 565 | Loss: 0.034053 | LR: 4.7413e-04 | Grad Norm: 0.1311\n",
      "Epoch 566 | Loss: 0.026875 | LR: 4.7497e-04 | Grad Norm: 0.0632\n",
      "Epoch 567 | Loss: 0.059269 | LR: 4.7580e-04 | Grad Norm: 0.1587\n",
      "Epoch 568 | Loss: 0.057082 | LR: 4.7664e-04 | Grad Norm: 0.0986\n",
      "Epoch 569 | Loss: 0.078214 | LR: 4.7748e-04 | Grad Norm: 0.1433\n",
      "Epoch 570 | Loss: 0.082637 | LR: 4.7832e-04 | Grad Norm: 0.1084\n",
      "Epoch 571 | Loss: 0.138692 | LR: 4.7916e-04 | Grad Norm: 0.1381\n",
      "Epoch 572 | Loss: 0.100339 | LR: 4.8000e-04 | Grad Norm: 0.1135\n",
      "Epoch 573 | Loss: 0.111029 | LR: 4.8084e-04 | Grad Norm: 0.2393\n",
      "Epoch 574 | Loss: 0.096138 | LR: 4.8168e-04 | Grad Norm: 0.4404\n",
      "Epoch 575 | Loss: 0.034573 | LR: 4.8252e-04 | Grad Norm: 0.0842\n",
      "Epoch 576 | Loss: 0.076747 | LR: 4.8336e-04 | Grad Norm: 0.2030\n",
      "Epoch 577 | Loss: 0.125431 | LR: 4.8420e-04 | Grad Norm: 0.2971\n",
      "Epoch 578 | Loss: 0.094678 | LR: 4.8503e-04 | Grad Norm: 0.1278\n",
      "Epoch 579 | Loss: 0.145971 | LR: 4.8587e-04 | Grad Norm: 0.2123\n",
      "Epoch 580 | Loss: 0.153128 | LR: 4.8671e-04 | Grad Norm: 0.5957\n",
      "Epoch 581 | Loss: 0.156204 | LR: 4.8755e-04 | Grad Norm: 0.2676\n",
      "Epoch 582 | Loss: 0.087486 | LR: 4.8839e-04 | Grad Norm: 0.1498\n",
      "Epoch 583 | Loss: 0.101750 | LR: 4.8923e-04 | Grad Norm: 0.2383\n",
      "Epoch 584 | Loss: 0.106763 | LR: 4.9007e-04 | Grad Norm: 0.4404\n",
      "Epoch 585 | Loss: 0.114072 | LR: 4.9091e-04 | Grad Norm: 0.2372\n",
      "Epoch 586 | Loss: 0.131875 | LR: 4.9175e-04 | Grad Norm: 0.3069\n",
      "Epoch 587 | Loss: 0.141737 | LR: 4.9259e-04 | Grad Norm: 0.1785\n",
      "Epoch 588 | Loss: 0.057144 | LR: 4.9343e-04 | Grad Norm: 0.2036\n",
      "Epoch 589 | Loss: 0.104783 | LR: 4.9427e-04 | Grad Norm: 0.1314\n",
      "Epoch 590 | Loss: 0.116731 | LR: 4.9510e-04 | Grad Norm: 0.2173\n",
      "Epoch 591 | Loss: 0.023893 | LR: 4.9594e-04 | Grad Norm: 0.0757\n",
      "Epoch 592 | Loss: 0.129587 | LR: 4.9678e-04 | Grad Norm: 0.6137\n",
      "Epoch 593 | Loss: 0.035644 | LR: 4.9762e-04 | Grad Norm: 0.0681\n",
      "Epoch 594 | Loss: 0.155952 | LR: 4.9846e-04 | Grad Norm: 0.3390\n",
      "Epoch 595 | Loss: 0.164859 | LR: 4.9930e-04 | Grad Norm: 0.1664\n",
      "Epoch 596 | Loss: 0.144688 | LR: 5.0014e-04 | Grad Norm: 0.3230\n",
      "Epoch 597 | Loss: 0.036495 | LR: 5.0098e-04 | Grad Norm: 0.1167\n",
      "Epoch 598 | Loss: 0.030667 | LR: 5.0182e-04 | Grad Norm: 0.0837\n",
      "Epoch 599 | Loss: 0.128656 | LR: 5.0266e-04 | Grad Norm: 0.1916\n",
      "Epoch 600 | Loss: 0.066115 | LR: 5.0350e-04 | Grad Norm: 0.1156\n",
      "Epoch 601 | Loss: 0.147670 | LR: 5.0434e-04 | Grad Norm: 0.2734\n",
      "Epoch 602 | Loss: 0.062912 | LR: 5.0517e-04 | Grad Norm: 0.2105\n",
      "Epoch 603 | Loss: 0.202203 | LR: 5.0601e-04 | Grad Norm: 0.2015\n",
      "Epoch 604 | Loss: 0.067815 | LR: 5.0685e-04 | Grad Norm: 0.2472\n",
      "Epoch 605 | Loss: 0.160003 | LR: 5.0769e-04 | Grad Norm: 0.2773\n",
      "Epoch 606 | Loss: 0.021532 | LR: 5.0853e-04 | Grad Norm: 0.2355\n",
      "Epoch 607 | Loss: 0.027517 | LR: 5.0937e-04 | Grad Norm: 0.0731\n",
      "Epoch 608 | Loss: 0.016476 | LR: 5.1021e-04 | Grad Norm: 0.1327\n",
      "Epoch 609 | Loss: 0.137140 | LR: 5.1105e-04 | Grad Norm: 0.1755\n",
      "Epoch 610 | Loss: 0.103552 | LR: 5.1189e-04 | Grad Norm: 0.3102\n",
      "Epoch 611 | Loss: 0.124880 | LR: 5.1273e-04 | Grad Norm: 0.2107\n",
      "Epoch 612 | Loss: 0.152479 | LR: 5.1357e-04 | Grad Norm: 0.2437\n",
      "Epoch 613 | Loss: 0.023167 | LR: 5.1441e-04 | Grad Norm: 0.0953\n",
      "Epoch 614 | Loss: 0.021580 | LR: 5.1524e-04 | Grad Norm: 0.1109\n",
      "Epoch 615 | Loss: 0.018927 | LR: 5.1608e-04 | Grad Norm: 0.0618\n",
      "Epoch 616 | Loss: 0.016584 | LR: 5.1692e-04 | Grad Norm: 0.0518\n",
      "Epoch 617 | Loss: 0.021703 | LR: 5.1776e-04 | Grad Norm: 0.1609\n",
      "Epoch 618 | Loss: 0.018284 | LR: 5.1860e-04 | Grad Norm: 0.0977\n",
      "Epoch 619 | Loss: 0.195049 | LR: 5.1944e-04 | Grad Norm: 0.4102\n",
      "Epoch 620 | Loss: 0.155640 | LR: 5.2028e-04 | Grad Norm: 0.2264\n",
      "Epoch 621 | Loss: 0.185109 | LR: 5.2112e-04 | Grad Norm: 0.2611\n",
      "Epoch 622 | Loss: 0.014796 | LR: 5.2196e-04 | Grad Norm: 0.0456\n",
      "Epoch 623 | Loss: 0.152616 | LR: 5.2280e-04 | Grad Norm: 0.2508\n",
      "Epoch 624 | Loss: 0.146899 | LR: 5.2364e-04 | Grad Norm: 0.3762\n",
      "Epoch 625 | Loss: 0.070682 | LR: 5.2448e-04 | Grad Norm: 0.2027\n",
      "Epoch 626 | Loss: 0.126534 | LR: 5.2531e-04 | Grad Norm: 0.2737\n",
      "Epoch 627 | Loss: 0.194057 | LR: 5.2615e-04 | Grad Norm: 0.3765\n",
      "Epoch 628 | Loss: 0.203867 | LR: 5.2699e-04 | Grad Norm: 0.2828\n",
      "Epoch 629 | Loss: 0.192320 | LR: 5.2783e-04 | Grad Norm: 0.2074\n",
      "Epoch 630 | Loss: 0.015222 | LR: 5.2867e-04 | Grad Norm: 0.0357\n",
      "Epoch 631 | Loss: 0.160815 | LR: 5.2951e-04 | Grad Norm: 0.1608\n",
      "Epoch 632 | Loss: 0.127622 | LR: 5.3035e-04 | Grad Norm: 0.2385\n",
      "Epoch 633 | Loss: 0.022904 | LR: 5.3119e-04 | Grad Norm: 0.0766\n",
      "Epoch 634 | Loss: 0.098220 | LR: 5.3203e-04 | Grad Norm: 0.1636\n",
      "Epoch 635 | Loss: 0.137177 | LR: 5.3287e-04 | Grad Norm: 0.1580\n",
      "Epoch 636 | Loss: 0.248341 | LR: 5.3371e-04 | Grad Norm: 1.8202\n",
      "Epoch 637 | Loss: 0.104712 | LR: 5.3455e-04 | Grad Norm: 0.1317\n",
      "Epoch 638 | Loss: 0.107061 | LR: 5.3538e-04 | Grad Norm: 0.5860\n",
      "Epoch 639 | Loss: 0.152285 | LR: 5.3622e-04 | Grad Norm: 0.3291\n",
      "Epoch 640 | Loss: 0.135113 | LR: 5.3706e-04 | Grad Norm: 0.2242\n",
      "Epoch 641 | Loss: 0.157304 | LR: 5.3790e-04 | Grad Norm: 0.2770\n",
      "Epoch 642 | Loss: 0.015452 | LR: 5.3874e-04 | Grad Norm: 0.0718\n",
      "Epoch 643 | Loss: 0.085383 | LR: 5.3958e-04 | Grad Norm: 0.1485\n",
      "Epoch 644 | Loss: 0.087748 | LR: 5.4042e-04 | Grad Norm: 0.1977\n",
      "Epoch 645 | Loss: 0.162079 | LR: 5.4126e-04 | Grad Norm: 0.1682\n",
      "Epoch 646 | Loss: 0.125153 | LR: 5.4210e-04 | Grad Norm: 0.1859\n",
      "Epoch 647 | Loss: 0.128820 | LR: 5.4294e-04 | Grad Norm: 0.2619\n",
      "Epoch 648 | Loss: 0.146371 | LR: 5.4378e-04 | Grad Norm: 0.1733\n",
      "Epoch 649 | Loss: 0.094997 | LR: 5.4462e-04 | Grad Norm: 0.2755\n",
      "Epoch 650 | Loss: 0.160267 | LR: 5.4545e-04 | Grad Norm: 0.2732\n",
      "Epoch 651 | Loss: 0.089751 | LR: 5.4629e-04 | Grad Norm: 1.2397\n",
      "Epoch 652 | Loss: 0.090236 | LR: 5.4713e-04 | Grad Norm: 0.1399\n",
      "Epoch 653 | Loss: 0.103876 | LR: 5.4797e-04 | Grad Norm: 0.3460\n",
      "Epoch 654 | Loss: 0.147814 | LR: 5.4881e-04 | Grad Norm: 0.4987\n",
      "Epoch 655 | Loss: 0.190847 | LR: 5.4965e-04 | Grad Norm: 0.2072\n",
      "Epoch 656 | Loss: 0.138945 | LR: 5.5049e-04 | Grad Norm: 0.1736\n",
      "Epoch 657 | Loss: 0.151980 | LR: 5.5133e-04 | Grad Norm: 0.5862\n",
      "Epoch 658 | Loss: 0.078556 | LR: 5.5217e-04 | Grad Norm: 0.1550\n",
      "Epoch 659 | Loss: 0.109488 | LR: 5.5301e-04 | Grad Norm: 0.1718\n",
      "Epoch 660 | Loss: 0.051989 | LR: 5.5385e-04 | Grad Norm: 0.1161\n",
      "Epoch 661 | Loss: 0.113705 | LR: 5.5469e-04 | Grad Norm: 0.2373\n",
      "Epoch 662 | Loss: 0.058313 | LR: 5.5552e-04 | Grad Norm: 0.1398\n",
      "Epoch 663 | Loss: 0.146452 | LR: 5.5636e-04 | Grad Norm: 0.2492\n",
      "Epoch 664 | Loss: 0.054116 | LR: 5.5720e-04 | Grad Norm: 0.1746\n",
      "Epoch 665 | Loss: 0.136893 | LR: 5.5804e-04 | Grad Norm: 0.1487\n",
      "Epoch 666 | Loss: 0.049092 | LR: 5.5888e-04 | Grad Norm: 0.0855\n",
      "Epoch 667 | Loss: 0.036770 | LR: 5.5972e-04 | Grad Norm: 0.1440\n",
      "Epoch 668 | Loss: 0.047198 | LR: 5.6056e-04 | Grad Norm: 0.1211\n",
      "Epoch 669 | Loss: 0.145833 | LR: 5.6140e-04 | Grad Norm: 0.1838\n",
      "Epoch 670 | Loss: 0.115994 | LR: 5.6224e-04 | Grad Norm: 0.1510\n",
      "Epoch 671 | Loss: 0.127027 | LR: 5.6308e-04 | Grad Norm: 0.1647\n",
      "Epoch 672 | Loss: 0.127742 | LR: 5.6392e-04 | Grad Norm: 0.1477\n",
      "Epoch 673 | Loss: 0.037999 | LR: 5.6476e-04 | Grad Norm: 0.0693\n",
      "Epoch 674 | Loss: 0.057387 | LR: 5.6559e-04 | Grad Norm: 0.1353\n",
      "Epoch 675 | Loss: 0.039797 | LR: 5.6643e-04 | Grad Norm: 0.0554\n",
      "Epoch 676 | Loss: 0.160384 | LR: 5.6727e-04 | Grad Norm: 0.3003\n",
      "Epoch 677 | Loss: 0.029721 | LR: 5.6811e-04 | Grad Norm: 0.0852\n",
      "Epoch 678 | Loss: 0.086956 | LR: 5.6895e-04 | Grad Norm: 0.2195\n",
      "Epoch 679 | Loss: 0.203767 | LR: 5.6979e-04 | Grad Norm: 0.4306\n",
      "Epoch 680 | Loss: 0.151444 | LR: 5.7063e-04 | Grad Norm: 0.3297\n",
      "Epoch 681 | Loss: 0.185103 | LR: 5.7147e-04 | Grad Norm: 0.3578\n",
      "Epoch 682 | Loss: 0.053587 | LR: 5.7231e-04 | Grad Norm: 0.1109\n",
      "Epoch 683 | Loss: 0.163344 | LR: 5.7315e-04 | Grad Norm: 0.4833\n",
      "Epoch 684 | Loss: 0.120015 | LR: 5.7399e-04 | Grad Norm: 0.1813\n",
      "Epoch 685 | Loss: 0.060187 | LR: 5.7483e-04 | Grad Norm: 0.2010\n",
      "Epoch 686 | Loss: 0.091015 | LR: 5.7566e-04 | Grad Norm: 0.1650\n",
      "Epoch 687 | Loss: 0.150685 | LR: 5.7650e-04 | Grad Norm: 0.1691\n",
      "Epoch 688 | Loss: 0.159954 | LR: 5.7734e-04 | Grad Norm: 0.1453\n",
      "Epoch 689 | Loss: 0.162500 | LR: 5.7818e-04 | Grad Norm: 0.1825\n",
      "Epoch 690 | Loss: 0.033224 | LR: 5.7902e-04 | Grad Norm: 0.1525\n",
      "Epoch 691 | Loss: 0.138449 | LR: 5.7986e-04 | Grad Norm: 0.1321\n",
      "Epoch 692 | Loss: 0.117523 | LR: 5.8070e-04 | Grad Norm: 0.1206\n",
      "Epoch 693 | Loss: 0.033446 | LR: 5.8154e-04 | Grad Norm: 0.1094\n",
      "Epoch 694 | Loss: 0.076421 | LR: 5.8238e-04 | Grad Norm: 0.1144\n",
      "Epoch 695 | Loss: 0.103579 | LR: 5.8322e-04 | Grad Norm: 0.1748\n",
      "Epoch 696 | Loss: 0.184882 | LR: 5.8406e-04 | Grad Norm: 0.5283\n",
      "Epoch 697 | Loss: 0.101444 | LR: 5.8490e-04 | Grad Norm: 0.1300\n",
      "Epoch 698 | Loss: 0.108264 | LR: 5.8573e-04 | Grad Norm: 0.1505\n",
      "Epoch 699 | Loss: 0.149555 | LR: 5.8657e-04 | Grad Norm: 0.2037\n",
      "Epoch 700 | Loss: 0.132201 | LR: 5.8741e-04 | Grad Norm: 0.1648\n",
      "Epoch 701 | Loss: 0.128857 | LR: 5.8825e-04 | Grad Norm: 0.1396\n",
      "Epoch 702 | Loss: 0.025581 | LR: 5.8909e-04 | Grad Norm: 0.0557\n",
      "Epoch 703 | Loss: 0.082116 | LR: 5.8993e-04 | Grad Norm: 0.1257\n",
      "Epoch 704 | Loss: 0.087798 | LR: 5.9077e-04 | Grad Norm: 0.2274\n",
      "Epoch 705 | Loss: 0.144260 | LR: 5.9161e-04 | Grad Norm: 0.1844\n",
      "Epoch 706 | Loss: 0.120239 | LR: 5.9245e-04 | Grad Norm: 0.1898\n",
      "Epoch 707 | Loss: 0.116715 | LR: 5.9329e-04 | Grad Norm: 0.2230\n",
      "Epoch 708 | Loss: 0.139844 | LR: 5.9413e-04 | Grad Norm: 0.4218\n",
      "Epoch 709 | Loss: 0.086748 | LR: 5.9497e-04 | Grad Norm: 0.1947\n",
      "Epoch 710 | Loss: 0.128624 | LR: 5.9580e-04 | Grad Norm: 0.1827\n",
      "Epoch 711 | Loss: 0.086518 | LR: 5.9664e-04 | Grad Norm: 0.1909\n",
      "Epoch 712 | Loss: 0.080511 | LR: 5.9748e-04 | Grad Norm: 0.1374\n",
      "Epoch 713 | Loss: 0.103805 | LR: 5.9832e-04 | Grad Norm: 0.1417\n",
      "Epoch 714 | Loss: 0.125661 | LR: 5.9916e-04 | Grad Norm: 0.1699\n",
      "Epoch 715 | Loss: 0.165114 | LR: 6.0000e-04 | Grad Norm: 0.2948\n",
      "Epoch 716 | Loss: 0.110405 | LR: 6.0000e-04 | Grad Norm: 0.1476\n",
      "Epoch 717 | Loss: 0.104974 | LR: 6.0000e-04 | Grad Norm: 0.1300\n",
      "Epoch 718 | Loss: 0.013736 | LR: 6.0000e-04 | Grad Norm: 0.0661\n",
      "Epoch 719 | Loss: 0.028240 | LR: 6.0000e-04 | Grad Norm: 0.0751\n",
      "Epoch 720 | Loss: 0.014155 | LR: 6.0000e-04 | Grad Norm: 0.1263\n",
      "Epoch 721 | Loss: 0.018452 | LR: 5.9999e-04 | Grad Norm: 0.0600\n",
      "Epoch 722 | Loss: 0.012863 | LR: 5.9999e-04 | Grad Norm: 0.0296\n",
      "Epoch 723 | Loss: 0.015524 | LR: 5.9999e-04 | Grad Norm: 0.0411\n",
      "Epoch 724 | Loss: 0.013387 | LR: 5.9998e-04 | Grad Norm: 0.0532\n",
      "Epoch 725 | Loss: 0.098661 | LR: 5.9998e-04 | Grad Norm: 0.1460\n",
      "Epoch 726 | Loss: 0.160406 | LR: 5.9997e-04 | Grad Norm: 0.2672\n",
      "Epoch 727 | Loss: 0.093224 | LR: 5.9997e-04 | Grad Norm: 0.1881\n",
      "Epoch 728 | Loss: 0.172020 | LR: 5.9996e-04 | Grad Norm: 0.2587\n",
      "Epoch 729 | Loss: 0.173009 | LR: 5.9996e-04 | Grad Norm: 0.2052\n",
      "Epoch 730 | Loss: 0.194214 | LR: 5.9995e-04 | Grad Norm: 0.3987\n",
      "Epoch 731 | Loss: 0.172347 | LR: 5.9994e-04 | Grad Norm: 0.2377\n",
      "Epoch 732 | Loss: 0.082535 | LR: 5.9993e-04 | Grad Norm: 0.1905\n",
      "Epoch 733 | Loss: 0.140343 | LR: 5.9993e-04 | Grad Norm: 0.3245\n",
      "Epoch 734 | Loss: 0.198987 | LR: 5.9992e-04 | Grad Norm: 0.3510\n",
      "Epoch 735 | Loss: 0.149324 | LR: 5.9991e-04 | Grad Norm: 0.2232\n",
      "Epoch 736 | Loss: 0.344973 | LR: 5.9990e-04 | Grad Norm: 0.3404\n",
      "Epoch 737 | Loss: 0.120454 | LR: 5.9989e-04 | Grad Norm: 0.1504\n",
      "Epoch 738 | Loss: 0.242510 | LR: 5.9988e-04 | Grad Norm: 0.2501\n",
      "Epoch 739 | Loss: 0.182756 | LR: 5.9987e-04 | Grad Norm: 0.2059\n",
      "Epoch 740 | Loss: 0.019147 | LR: 5.9985e-04 | Grad Norm: 0.1454\n",
      "Epoch 741 | Loss: 0.105282 | LR: 5.9984e-04 | Grad Norm: 0.1458\n",
      "Epoch 742 | Loss: 0.177811 | LR: 5.9983e-04 | Grad Norm: 0.1926\n",
      "Epoch 743 | Loss: 0.169301 | LR: 5.9981e-04 | Grad Norm: 0.1966\n",
      "Epoch 744 | Loss: 0.027060 | LR: 5.9980e-04 | Grad Norm: 0.1897\n",
      "Epoch 745 | Loss: 0.013089 | LR: 5.9979e-04 | Grad Norm: 0.0297\n",
      "Epoch 746 | Loss: 0.016498 | LR: 5.9977e-04 | Grad Norm: 0.0522\n",
      "Epoch 747 | Loss: 0.016107 | LR: 5.9975e-04 | Grad Norm: 0.1705\n",
      "Epoch 748 | Loss: 0.021890 | LR: 5.9974e-04 | Grad Norm: 0.0742\n",
      "Epoch 749 | Loss: 0.087815 | LR: 5.9972e-04 | Grad Norm: 0.1565\n",
      "Epoch 750 | Loss: 0.113197 | LR: 5.9971e-04 | Grad Norm: 0.1714\n",
      "Epoch 751 | Loss: 0.091236 | LR: 5.9969e-04 | Grad Norm: 0.1542\n",
      "Epoch 752 | Loss: 0.128960 | LR: 5.9967e-04 | Grad Norm: 0.1455\n",
      "Epoch 753 | Loss: 0.096902 | LR: 5.9965e-04 | Grad Norm: 0.4311\n",
      "Epoch 754 | Loss: 0.018889 | LR: 5.9963e-04 | Grad Norm: 0.2774\n",
      "Epoch 755 | Loss: 0.018600 | LR: 5.9961e-04 | Grad Norm: 0.1045\n",
      "Epoch 756 | Loss: 0.127825 | LR: 5.9959e-04 | Grad Norm: 0.1921\n",
      "Epoch 757 | Loss: 0.085478 | LR: 5.9957e-04 | Grad Norm: 0.1352\n",
      "Epoch 758 | Loss: 0.158371 | LR: 5.9955e-04 | Grad Norm: 0.3184\n",
      "Epoch 759 | Loss: 0.156225 | LR: 5.9953e-04 | Grad Norm: 0.3032\n",
      "Epoch 760 | Loss: 0.149749 | LR: 5.9951e-04 | Grad Norm: 0.1971\n",
      "Epoch 761 | Loss: 0.092734 | LR: 5.9948e-04 | Grad Norm: 0.1675\n",
      "Epoch 762 | Loss: 0.099611 | LR: 5.9946e-04 | Grad Norm: 0.1575\n",
      "Epoch 763 | Loss: 0.095575 | LR: 5.9944e-04 | Grad Norm: 0.1435\n",
      "Epoch 764 | Loss: 0.097039 | LR: 5.9941e-04 | Grad Norm: 0.1433\n",
      "Epoch 765 | Loss: 0.142749 | LR: 5.9939e-04 | Grad Norm: 0.2566\n",
      "Epoch 766 | Loss: 0.130603 | LR: 5.9936e-04 | Grad Norm: 0.2430\n",
      "Epoch 767 | Loss: 0.018348 | LR: 5.9934e-04 | Grad Norm: 0.0831\n",
      "Epoch 768 | Loss: 0.110630 | LR: 5.9931e-04 | Grad Norm: 0.2052\n",
      "Epoch 769 | Loss: 0.087424 | LR: 5.9928e-04 | Grad Norm: 0.1987\n",
      "Epoch 770 | Loss: 0.018217 | LR: 5.9926e-04 | Grad Norm: 0.1096\n",
      "Epoch 771 | Loss: 0.125171 | LR: 5.9923e-04 | Grad Norm: 0.2180\n",
      "Epoch 772 | Loss: 0.026416 | LR: 5.9920e-04 | Grad Norm: 0.2277\n",
      "Epoch 773 | Loss: 0.168636 | LR: 5.9917e-04 | Grad Norm: 0.2481\n",
      "Epoch 774 | Loss: 0.140689 | LR: 5.9914e-04 | Grad Norm: 0.7504\n",
      "Epoch 775 | Loss: 0.141341 | LR: 5.9911e-04 | Grad Norm: 0.5783\n",
      "Epoch 776 | Loss: 0.013656 | LR: 5.9908e-04 | Grad Norm: 0.0318\n",
      "Epoch 777 | Loss: 0.016812 | LR: 5.9905e-04 | Grad Norm: 0.0512\n",
      "Epoch 778 | Loss: 0.029001 | LR: 5.9902e-04 | Grad Norm: 0.0572\n",
      "Epoch 779 | Loss: 0.026654 | LR: 5.9899e-04 | Grad Norm: 0.0933\n",
      "Epoch 780 | Loss: 0.030821 | LR: 5.9896e-04 | Grad Norm: 0.1628\n",
      "Epoch 781 | Loss: 0.018514 | LR: 5.9892e-04 | Grad Norm: 0.0712\n",
      "Epoch 782 | Loss: 0.051796 | LR: 5.9889e-04 | Grad Norm: 0.1113\n",
      "Epoch 783 | Loss: 0.017501 | LR: 5.9886e-04 | Grad Norm: 0.0353\n",
      "Epoch 784 | Loss: 0.040751 | LR: 5.9882e-04 | Grad Norm: 0.0612\n",
      "Epoch 785 | Loss: 0.085995 | LR: 5.9879e-04 | Grad Norm: 0.1425\n",
      "Epoch 786 | Loss: 0.130314 | LR: 5.9875e-04 | Grad Norm: 0.1848\n",
      "Epoch 787 | Loss: 0.084792 | LR: 5.9871e-04 | Grad Norm: 0.1809\n",
      "Epoch 788 | Loss: 0.156391 | LR: 5.9868e-04 | Grad Norm: 0.2041\n",
      "Epoch 789 | Loss: 0.140352 | LR: 5.9864e-04 | Grad Norm: 0.1554\n",
      "Epoch 790 | Loss: 0.155653 | LR: 5.9860e-04 | Grad Norm: 0.1817\n",
      "Epoch 791 | Loss: 0.148596 | LR: 5.9857e-04 | Grad Norm: 0.2757\n",
      "Epoch 792 | Loss: 0.063977 | LR: 5.9853e-04 | Grad Norm: 0.1231\n",
      "Epoch 793 | Loss: 0.105583 | LR: 5.9849e-04 | Grad Norm: 0.1334\n",
      "Epoch 794 | Loss: 0.136213 | LR: 5.9845e-04 | Grad Norm: 0.2040\n",
      "Epoch 795 | Loss: 0.111646 | LR: 5.9841e-04 | Grad Norm: 0.1510\n",
      "Epoch 796 | Loss: 0.217164 | LR: 5.9837e-04 | Grad Norm: 0.2190\n",
      "Epoch 797 | Loss: 0.091559 | LR: 5.9833e-04 | Grad Norm: 0.1261\n",
      "Epoch 798 | Loss: 0.218394 | LR: 5.9829e-04 | Grad Norm: 0.2847\n",
      "Epoch 799 | Loss: 0.169365 | LR: 5.9824e-04 | Grad Norm: 0.1577\n",
      "Epoch 800 | Loss: 0.051972 | LR: 5.9820e-04 | Grad Norm: 0.1122\n",
      "Epoch 801 | Loss: 0.090809 | LR: 5.9816e-04 | Grad Norm: 0.1207\n",
      "Epoch 802 | Loss: 0.173996 | LR: 5.9811e-04 | Grad Norm: 0.2136\n",
      "Epoch 803 | Loss: 0.162737 | LR: 5.9807e-04 | Grad Norm: 0.1951\n",
      "Epoch 804 | Loss: 0.024217 | LR: 5.9803e-04 | Grad Norm: 0.0908\n",
      "Epoch 805 | Loss: 0.023042 | LR: 5.9798e-04 | Grad Norm: 0.0792\n",
      "Epoch 806 | Loss: 0.050343 | LR: 5.9794e-04 | Grad Norm: 0.0949\n",
      "Epoch 807 | Loss: 0.049071 | LR: 5.9789e-04 | Grad Norm: 0.1608\n",
      "Epoch 808 | Loss: 0.067649 | LR: 5.9784e-04 | Grad Norm: 0.1291\n",
      "Epoch 809 | Loss: 0.075203 | LR: 5.9780e-04 | Grad Norm: 0.1080\n",
      "Epoch 810 | Loss: 0.116681 | LR: 5.9775e-04 | Grad Norm: 0.1315\n",
      "Epoch 811 | Loss: 0.089910 | LR: 5.9770e-04 | Grad Norm: 0.1198\n",
      "Epoch 812 | Loss: 0.108187 | LR: 5.9765e-04 | Grad Norm: 0.1587\n",
      "Epoch 813 | Loss: 0.083983 | LR: 5.9760e-04 | Grad Norm: 0.1360\n",
      "Epoch 814 | Loss: 0.027632 | LR: 5.9755e-04 | Grad Norm: 0.0416\n",
      "Epoch 815 | Loss: 0.060887 | LR: 5.9750e-04 | Grad Norm: 0.1247\n",
      "Epoch 816 | Loss: 0.114114 | LR: 5.9745e-04 | Grad Norm: 0.1305\n",
      "Epoch 817 | Loss: 0.087052 | LR: 5.9740e-04 | Grad Norm: 0.1349\n",
      "Epoch 818 | Loss: 0.139185 | LR: 5.9735e-04 | Grad Norm: 0.1711\n",
      "Epoch 819 | Loss: 0.156717 | LR: 5.9730e-04 | Grad Norm: 0.1831\n",
      "Epoch 820 | Loss: 0.149227 | LR: 5.9724e-04 | Grad Norm: 0.2020\n",
      "Epoch 821 | Loss: 0.077813 | LR: 5.9719e-04 | Grad Norm: 0.1425\n",
      "Epoch 822 | Loss: 0.080095 | LR: 5.9714e-04 | Grad Norm: 0.1383\n",
      "Epoch 823 | Loss: 0.077544 | LR: 5.9708e-04 | Grad Norm: 0.1198\n",
      "Epoch 824 | Loss: 0.109229 | LR: 5.9703e-04 | Grad Norm: 0.1744\n",
      "Epoch 825 | Loss: 0.123524 | LR: 5.9697e-04 | Grad Norm: 0.1849\n",
      "Epoch 826 | Loss: 0.127070 | LR: 5.9692e-04 | Grad Norm: 0.1436\n",
      "Epoch 827 | Loss: 0.040304 | LR: 5.9686e-04 | Grad Norm: 0.0857\n",
      "Epoch 828 | Loss: 0.100296 | LR: 5.9681e-04 | Grad Norm: 0.1292\n",
      "Epoch 829 | Loss: 0.104131 | LR: 5.9675e-04 | Grad Norm: 0.1828\n",
      "Epoch 830 | Loss: 0.018061 | LR: 5.9669e-04 | Grad Norm: 0.0412\n",
      "Epoch 831 | Loss: 0.117573 | LR: 5.9663e-04 | Grad Norm: 0.1982\n",
      "Epoch 832 | Loss: 0.031308 | LR: 5.9657e-04 | Grad Norm: 0.0644\n",
      "Epoch 833 | Loss: 0.144273 | LR: 5.9651e-04 | Grad Norm: 0.2205\n",
      "Epoch 834 | Loss: 0.139353 | LR: 5.9645e-04 | Grad Norm: 0.1646\n",
      "Epoch 835 | Loss: 0.124600 | LR: 5.9639e-04 | Grad Norm: 0.1483\n",
      "Epoch 836 | Loss: 0.028595 | LR: 5.9633e-04 | Grad Norm: 0.0742\n",
      "Epoch 837 | Loss: 0.023308 | LR: 5.9627e-04 | Grad Norm: 0.0670\n",
      "Epoch 838 | Loss: 0.131727 | LR: 5.9621e-04 | Grad Norm: 0.1936\n",
      "Epoch 839 | Loss: 0.072167 | LR: 5.9615e-04 | Grad Norm: 0.2250\n",
      "Epoch 840 | Loss: 0.130680 | LR: 5.9609e-04 | Grad Norm: 0.2429\n",
      "Epoch 841 | Loss: 0.064829 | LR: 5.9602e-04 | Grad Norm: 0.1529\n",
      "Epoch 842 | Loss: 0.183183 | LR: 5.9596e-04 | Grad Norm: 0.2153\n",
      "Epoch 843 | Loss: 0.057248 | LR: 5.9589e-04 | Grad Norm: 0.1899\n",
      "Epoch 844 | Loss: 0.146181 | LR: 5.9583e-04 | Grad Norm: 0.2476\n",
      "Epoch 845 | Loss: 0.014732 | LR: 5.9576e-04 | Grad Norm: 0.0783\n",
      "Epoch 846 | Loss: 0.025495 | LR: 5.9570e-04 | Grad Norm: 0.2972\n",
      "Epoch 847 | Loss: 0.012616 | LR: 5.9563e-04 | Grad Norm: 0.0322\n",
      "Epoch 848 | Loss: 0.125731 | LR: 5.9557e-04 | Grad Norm: 0.1923\n",
      "Epoch 849 | Loss: 0.088769 | LR: 5.9550e-04 | Grad Norm: 0.1344\n",
      "Epoch 850 | Loss: 0.109653 | LR: 5.9543e-04 | Grad Norm: 0.1530\n",
      "Epoch 851 | Loss: 0.137157 | LR: 5.9536e-04 | Grad Norm: 0.1601\n",
      "Epoch 852 | Loss: 0.019449 | LR: 5.9529e-04 | Grad Norm: 0.0856\n",
      "Epoch 853 | Loss: 0.025260 | LR: 5.9522e-04 | Grad Norm: 0.1297\n",
      "Epoch 854 | Loss: 0.016562 | LR: 5.9515e-04 | Grad Norm: 0.1242\n",
      "Epoch 855 | Loss: 0.012009 | LR: 5.9508e-04 | Grad Norm: 0.0473\n",
      "Epoch 856 | Loss: 0.018737 | LR: 5.9501e-04 | Grad Norm: 0.2032\n",
      "Epoch 857 | Loss: 0.013822 | LR: 5.9494e-04 | Grad Norm: 0.0387\n",
      "Epoch 858 | Loss: 0.181864 | LR: 5.9487e-04 | Grad Norm: 0.2270\n",
      "Epoch 859 | Loss: 0.150815 | LR: 5.9480e-04 | Grad Norm: 0.1969\n",
      "Epoch 860 | Loss: 0.181097 | LR: 5.9473e-04 | Grad Norm: 0.2276\n",
      "Epoch 861 | Loss: 0.012249 | LR: 5.9465e-04 | Grad Norm: 0.0307\n",
      "Epoch 862 | Loss: 0.157174 | LR: 5.9458e-04 | Grad Norm: 0.1981\n",
      "Epoch 863 | Loss: 0.139412 | LR: 5.9450e-04 | Grad Norm: 0.2656\n",
      "Epoch 864 | Loss: 0.051013 | LR: 5.9443e-04 | Grad Norm: 0.1345\n",
      "Epoch 865 | Loss: 0.093091 | LR: 5.9435e-04 | Grad Norm: 0.2083\n",
      "Epoch 866 | Loss: 0.190224 | LR: 5.9428e-04 | Grad Norm: 0.2022\n",
      "Epoch 867 | Loss: 0.217835 | LR: 5.9420e-04 | Grad Norm: 0.3786\n",
      "Epoch 868 | Loss: 0.189260 | LR: 5.9413e-04 | Grad Norm: 0.3285\n",
      "Epoch 869 | Loss: 0.014732 | LR: 5.9405e-04 | Grad Norm: 0.0838\n",
      "Epoch 870 | Loss: 0.135711 | LR: 5.9397e-04 | Grad Norm: 0.1544\n",
      "Epoch 871 | Loss: 0.113692 | LR: 5.9389e-04 | Grad Norm: 0.1630\n",
      "Epoch 872 | Loss: 0.018602 | LR: 5.9381e-04 | Grad Norm: 0.0848\n",
      "Epoch 873 | Loss: 0.085377 | LR: 5.9373e-04 | Grad Norm: 0.1212\n",
      "Epoch 874 | Loss: 0.126957 | LR: 5.9365e-04 | Grad Norm: 0.2211\n",
      "Epoch 875 | Loss: 0.233559 | LR: 5.9357e-04 | Grad Norm: 0.2859\n",
      "Epoch 876 | Loss: 0.102585 | LR: 5.9349e-04 | Grad Norm: 0.1341\n",
      "Epoch 877 | Loss: 0.108425 | LR: 5.9341e-04 | Grad Norm: 0.2092\n",
      "Epoch 878 | Loss: 0.131588 | LR: 5.9333e-04 | Grad Norm: 0.1870\n",
      "Epoch 879 | Loss: 0.134909 | LR: 5.9325e-04 | Grad Norm: 0.4292\n",
      "Epoch 880 | Loss: 0.141690 | LR: 5.9317e-04 | Grad Norm: 0.3413\n",
      "Epoch 881 | Loss: 0.013795 | LR: 5.9308e-04 | Grad Norm: 0.0862\n",
      "Epoch 882 | Loss: 0.080632 | LR: 5.9300e-04 | Grad Norm: 0.1566\n",
      "Epoch 883 | Loss: 0.083322 | LR: 5.9291e-04 | Grad Norm: 0.2008\n",
      "Epoch 884 | Loss: 0.146450 | LR: 5.9283e-04 | Grad Norm: 0.1716\n",
      "Epoch 885 | Loss: 0.112299 | LR: 5.9274e-04 | Grad Norm: 0.3795\n",
      "Epoch 886 | Loss: 0.126856 | LR: 5.9266e-04 | Grad Norm: 0.1846\n",
      "Epoch 887 | Loss: 0.161451 | LR: 5.9257e-04 | Grad Norm: 0.2413\n",
      "Epoch 888 | Loss: 0.103402 | LR: 5.9249e-04 | Grad Norm: 0.2015\n",
      "Epoch 889 | Loss: 0.175217 | LR: 5.9240e-04 | Grad Norm: 0.2706\n",
      "Epoch 890 | Loss: 0.072825 | LR: 5.9231e-04 | Grad Norm: 0.1316\n",
      "Epoch 891 | Loss: 0.090392 | LR: 5.9222e-04 | Grad Norm: 0.1336\n",
      "Epoch 892 | Loss: 0.100025 | LR: 5.9213e-04 | Grad Norm: 0.1633\n",
      "Epoch 893 | Loss: 0.124484 | LR: 5.9204e-04 | Grad Norm: 0.1781\n",
      "Epoch 894 | Loss: 0.165699 | LR: 5.9195e-04 | Grad Norm: 0.2023\n",
      "Epoch 895 | Loss: 0.133317 | LR: 5.9186e-04 | Grad Norm: 0.1835\n",
      "Epoch 896 | Loss: 0.120116 | LR: 5.9177e-04 | Grad Norm: 0.2281\n",
      "Epoch 897 | Loss: 0.078511 | LR: 5.9168e-04 | Grad Norm: 0.1486\n",
      "Epoch 898 | Loss: 0.110864 | LR: 5.9159e-04 | Grad Norm: 0.1701\n",
      "Epoch 899 | Loss: 0.056770 | LR: 5.9150e-04 | Grad Norm: 0.1804\n",
      "Epoch 900 | Loss: 0.102269 | LR: 5.9141e-04 | Grad Norm: 0.2017\n",
      "Epoch 901 | Loss: 0.051480 | LR: 5.9131e-04 | Grad Norm: 0.1493\n",
      "Epoch 902 | Loss: 0.146411 | LR: 5.9122e-04 | Grad Norm: 0.4229\n",
      "Epoch 903 | Loss: 0.050665 | LR: 5.9113e-04 | Grad Norm: 0.1489\n",
      "Epoch 904 | Loss: 0.124927 | LR: 5.9103e-04 | Grad Norm: 0.1796\n",
      "Epoch 905 | Loss: 0.042864 | LR: 5.9094e-04 | Grad Norm: 0.0985\n",
      "Epoch 906 | Loss: 0.031421 | LR: 5.9084e-04 | Grad Norm: 0.1748\n",
      "Epoch 907 | Loss: 0.044715 | LR: 5.9074e-04 | Grad Norm: 0.0923\n",
      "Epoch 908 | Loss: 0.130064 | LR: 5.9065e-04 | Grad Norm: 0.1663\n",
      "Epoch 909 | Loss: 0.109666 | LR: 5.9055e-04 | Grad Norm: 0.2186\n",
      "Epoch 910 | Loss: 0.116299 | LR: 5.9045e-04 | Grad Norm: 0.2157\n",
      "Epoch 911 | Loss: 0.126257 | LR: 5.9035e-04 | Grad Norm: 0.2037\n",
      "Epoch 912 | Loss: 0.033941 | LR: 5.9026e-04 | Grad Norm: 0.0587\n",
      "Epoch 913 | Loss: 0.061424 | LR: 5.9016e-04 | Grad Norm: 0.1291\n",
      "Epoch 914 | Loss: 0.041811 | LR: 5.9006e-04 | Grad Norm: 0.0749\n",
      "Epoch 915 | Loss: 0.159081 | LR: 5.8996e-04 | Grad Norm: 0.2411\n",
      "Epoch 916 | Loss: 0.030045 | LR: 5.8986e-04 | Grad Norm: 0.1595\n",
      "Epoch 917 | Loss: 0.070789 | LR: 5.8976e-04 | Grad Norm: 0.1221\n",
      "Epoch 918 | Loss: 0.192010 | LR: 5.8965e-04 | Grad Norm: 0.1685\n",
      "Epoch 919 | Loss: 0.128375 | LR: 5.8955e-04 | Grad Norm: 0.3016\n",
      "Epoch 920 | Loss: 0.158498 | LR: 5.8945e-04 | Grad Norm: 0.2395\n",
      "Epoch 921 | Loss: 0.044618 | LR: 5.8935e-04 | Grad Norm: 0.0533\n",
      "Epoch 922 | Loss: 0.144914 | LR: 5.8924e-04 | Grad Norm: 0.2072\n",
      "Epoch 923 | Loss: 0.108753 | LR: 5.8914e-04 | Grad Norm: 0.2352\n",
      "Epoch 924 | Loss: 0.050592 | LR: 5.8903e-04 | Grad Norm: 0.1641\n",
      "Epoch 925 | Loss: 0.077853 | LR: 5.8893e-04 | Grad Norm: 0.1295\n",
      "Epoch 926 | Loss: 0.149832 | LR: 5.8882e-04 | Grad Norm: 0.1731\n",
      "Epoch 927 | Loss: 0.153383 | LR: 5.8872e-04 | Grad Norm: 0.2032\n",
      "Epoch 928 | Loss: 0.150283 | LR: 5.8861e-04 | Grad Norm: 0.2148\n",
      "Epoch 929 | Loss: 0.029035 | LR: 5.8850e-04 | Grad Norm: 0.0834\n",
      "Epoch 930 | Loss: 0.124909 | LR: 5.8840e-04 | Grad Norm: 0.1529\n",
      "Epoch 931 | Loss: 0.117298 | LR: 5.8829e-04 | Grad Norm: 0.3533\n",
      "Epoch 932 | Loss: 0.026223 | LR: 5.8818e-04 | Grad Norm: 0.0904\n",
      "Epoch 933 | Loss: 0.067463 | LR: 5.8807e-04 | Grad Norm: 0.1167\n",
      "Epoch 934 | Loss: 0.097419 | LR: 5.8796e-04 | Grad Norm: 0.1420\n",
      "Epoch 935 | Loss: 0.191936 | LR: 5.8785e-04 | Grad Norm: 0.2318\n",
      "Epoch 936 | Loss: 0.089620 | LR: 5.8774e-04 | Grad Norm: 0.1629\n",
      "Epoch 937 | Loss: 0.102462 | LR: 5.8763e-04 | Grad Norm: 0.1481\n",
      "Epoch 938 | Loss: 0.136760 | LR: 5.8752e-04 | Grad Norm: 0.1742\n",
      "Epoch 939 | Loss: 0.139781 | LR: 5.8741e-04 | Grad Norm: 0.1699\n",
      "Epoch 940 | Loss: 0.124371 | LR: 5.8730e-04 | Grad Norm: 0.1869\n",
      "Epoch 941 | Loss: 0.025646 | LR: 5.8718e-04 | Grad Norm: 0.0803\n",
      "Epoch 942 | Loss: 0.073015 | LR: 5.8707e-04 | Grad Norm: 0.1171\n",
      "Epoch 943 | Loss: 0.076160 | LR: 5.8696e-04 | Grad Norm: 0.1609\n",
      "Epoch 944 | Loss: 0.129506 | LR: 5.8684e-04 | Grad Norm: 0.1612\n",
      "Epoch 945 | Loss: 0.107357 | LR: 5.8673e-04 | Grad Norm: 0.1725\n",
      "Epoch 946 | Loss: 0.101114 | LR: 5.8661e-04 | Grad Norm: 0.1811\n",
      "Epoch 947 | Loss: 0.123347 | LR: 5.8650e-04 | Grad Norm: 0.1522\n",
      "Epoch 948 | Loss: 0.084511 | LR: 5.8638e-04 | Grad Norm: 0.1451\n",
      "Epoch 949 | Loss: 0.127321 | LR: 5.8626e-04 | Grad Norm: 0.1536\n",
      "Epoch 950 | Loss: 0.086507 | LR: 5.8615e-04 | Grad Norm: 0.1410\n",
      "Epoch 951 | Loss: 0.071489 | LR: 5.8603e-04 | Grad Norm: 0.1414\n",
      "Epoch 952 | Loss: 0.111773 | LR: 5.8591e-04 | Grad Norm: 0.2686\n",
      "Epoch 953 | Loss: 0.121259 | LR: 5.8579e-04 | Grad Norm: 0.1842\n",
      "Epoch 954 | Loss: 0.148644 | LR: 5.8567e-04 | Grad Norm: 0.1644\n",
      "Epoch 955 | Loss: 0.102436 | LR: 5.8555e-04 | Grad Norm: 0.1553\n",
      "Epoch 956 | Loss: 0.101806 | LR: 5.8543e-04 | Grad Norm: 0.2159\n",
      "Epoch 957 | Loss: 0.010824 | LR: 5.8531e-04 | Grad Norm: 0.0303\n",
      "Epoch 958 | Loss: 0.023314 | LR: 5.8519e-04 | Grad Norm: 0.1348\n",
      "Epoch 959 | Loss: 0.009948 | LR: 5.8507e-04 | Grad Norm: 0.0190\n",
      "Epoch 960 | Loss: 0.016216 | LR: 5.8495e-04 | Grad Norm: 0.0630\n",
      "Epoch 961 | Loss: 0.011697 | LR: 5.8483e-04 | Grad Norm: 0.0731\n",
      "Epoch 962 | Loss: 0.014425 | LR: 5.8470e-04 | Grad Norm: 0.0410\n",
      "Epoch 963 | Loss: 0.012562 | LR: 5.8458e-04 | Grad Norm: 0.0387\n",
      "Epoch 964 | Loss: 0.125578 | LR: 5.8446e-04 | Grad Norm: 0.1865\n",
      "Epoch 965 | Loss: 0.174487 | LR: 5.8433e-04 | Grad Norm: 0.2603\n",
      "Epoch 966 | Loss: 0.089591 | LR: 5.8421e-04 | Grad Norm: 0.1286\n",
      "Epoch 967 | Loss: 0.191710 | LR: 5.8408e-04 | Grad Norm: 0.2699\n",
      "Epoch 968 | Loss: 0.166928 | LR: 5.8396e-04 | Grad Norm: 0.2212\n",
      "Epoch 969 | Loss: 0.169769 | LR: 5.8383e-04 | Grad Norm: 0.1942\n",
      "Epoch 970 | Loss: 0.152000 | LR: 5.8370e-04 | Grad Norm: 0.1854\n",
      "Epoch 971 | Loss: 0.070893 | LR: 5.8358e-04 | Grad Norm: 0.1222\n",
      "Epoch 972 | Loss: 0.133005 | LR: 5.8345e-04 | Grad Norm: 0.1646\n",
      "Epoch 973 | Loss: 0.166946 | LR: 5.8332e-04 | Grad Norm: 0.2076\n",
      "Epoch 974 | Loss: 0.139156 | LR: 5.8319e-04 | Grad Norm: 0.2710\n",
      "Epoch 975 | Loss: 0.322469 | LR: 5.8306e-04 | Grad Norm: 0.3851\n",
      "Epoch 976 | Loss: 0.107665 | LR: 5.8293e-04 | Grad Norm: 0.4003\n",
      "Epoch 977 | Loss: 0.242060 | LR: 5.8280e-04 | Grad Norm: 0.3390\n",
      "Epoch 978 | Loss: 0.195999 | LR: 5.8267e-04 | Grad Norm: 0.2559\n",
      "Epoch 979 | Loss: 0.014126 | LR: 5.8254e-04 | Grad Norm: 0.1045\n",
      "Epoch 980 | Loss: 0.112372 | LR: 5.8241e-04 | Grad Norm: 0.3044\n",
      "Epoch 981 | Loss: 0.175854 | LR: 5.8228e-04 | Grad Norm: 0.3480\n",
      "Epoch 982 | Loss: 0.177981 | LR: 5.8214e-04 | Grad Norm: 0.2275\n",
      "Epoch 983 | Loss: 0.019324 | LR: 5.8201e-04 | Grad Norm: 0.0785\n",
      "Epoch 984 | Loss: 0.011554 | LR: 5.8188e-04 | Grad Norm: 0.0385\n",
      "Epoch 985 | Loss: 0.012137 | LR: 5.8174e-04 | Grad Norm: 0.0363\n",
      "Epoch 986 | Loss: 0.017962 | LR: 5.8161e-04 | Grad Norm: 0.3027\n",
      "Epoch 987 | Loss: 0.017649 | LR: 5.8147e-04 | Grad Norm: 0.0618\n",
      "Epoch 988 | Loss: 0.078148 | LR: 5.8134e-04 | Grad Norm: 0.1380\n",
      "Epoch 989 | Loss: 0.118350 | LR: 5.8120e-04 | Grad Norm: 0.1781\n",
      "Epoch 990 | Loss: 0.085450 | LR: 5.8107e-04 | Grad Norm: 0.1473\n",
      "Epoch 991 | Loss: 0.110653 | LR: 5.8093e-04 | Grad Norm: 0.1709\n",
      "Epoch 992 | Loss: 0.090461 | LR: 5.8079e-04 | Grad Norm: 0.1539\n",
      "Epoch 993 | Loss: 0.011031 | LR: 5.8066e-04 | Grad Norm: 0.0230\n",
      "Epoch 994 | Loss: 0.013919 | LR: 5.8052e-04 | Grad Norm: 0.0345\n",
      "Epoch 995 | Loss: 0.111235 | LR: 5.8038e-04 | Grad Norm: 0.2135\n",
      "Epoch 996 | Loss: 0.080426 | LR: 5.8024e-04 | Grad Norm: 0.1475\n",
      "Epoch 997 | Loss: 0.126664 | LR: 5.8010e-04 | Grad Norm: 0.1906\n",
      "Epoch 998 | Loss: 0.145226 | LR: 5.7996e-04 | Grad Norm: 0.3074\n",
      "Epoch 999 | Loss: 0.143427 | LR: 5.7982e-04 | Grad Norm: 0.2594\n",
      "Epoch 1000 | Loss: 0.089576 | LR: 5.7968e-04 | Grad Norm: 0.1560\n",
      "Epoch 1001 | Loss: 0.088688 | LR: 5.7954e-04 | Grad Norm: 0.1738\n",
      "Epoch 1002 | Loss: 0.081151 | LR: 5.7939e-04 | Grad Norm: 0.1428\n",
      "Epoch 1003 | Loss: 0.097219 | LR: 5.7925e-04 | Grad Norm: 0.1844\n",
      "Epoch 1004 | Loss: 0.114791 | LR: 5.7911e-04 | Grad Norm: 0.3567\n",
      "Epoch 1005 | Loss: 0.128512 | LR: 5.7897e-04 | Grad Norm: 0.2161\n",
      "Epoch 1006 | Loss: 0.013150 | LR: 5.7882e-04 | Grad Norm: 0.0511\n",
      "Epoch 1007 | Loss: 0.102728 | LR: 5.7868e-04 | Grad Norm: 0.2144\n",
      "Epoch 1008 | Loss: 0.091542 | LR: 5.7853e-04 | Grad Norm: 0.2342\n",
      "Epoch 1009 | Loss: 0.012064 | LR: 5.7839e-04 | Grad Norm: 0.0606\n",
      "Epoch 1010 | Loss: 0.138692 | LR: 5.7824e-04 | Grad Norm: 0.1787\n",
      "Epoch 1011 | Loss: 0.015780 | LR: 5.7809e-04 | Grad Norm: 0.1684\n",
      "Epoch 1012 | Loss: 0.153957 | LR: 5.7795e-04 | Grad Norm: 0.3641\n",
      "Epoch 1013 | Loss: 0.125619 | LR: 5.7780e-04 | Grad Norm: 0.1948\n",
      "Epoch 1014 | Loss: 0.118798 | LR: 5.7765e-04 | Grad Norm: 0.2205\n",
      "Epoch 1015 | Loss: 0.014991 | LR: 5.7751e-04 | Grad Norm: 0.1270\n",
      "Epoch 1016 | Loss: 0.013281 | LR: 5.7736e-04 | Grad Norm: 0.0603\n",
      "Epoch 1017 | Loss: 0.028340 | LR: 5.7721e-04 | Grad Norm: 0.0789\n",
      "Epoch 1018 | Loss: 0.021471 | LR: 5.7706e-04 | Grad Norm: 0.0662\n",
      "Epoch 1019 | Loss: 0.028979 | LR: 5.7691e-04 | Grad Norm: 0.0929\n",
      "Epoch 1020 | Loss: 0.016731 | LR: 5.7676e-04 | Grad Norm: 0.0427\n",
      "Epoch 1021 | Loss: 0.046923 | LR: 5.7661e-04 | Grad Norm: 0.0964\n",
      "Epoch 1022 | Loss: 0.017155 | LR: 5.7646e-04 | Grad Norm: 0.0656\n",
      "Epoch 1023 | Loss: 0.031460 | LR: 5.7630e-04 | Grad Norm: 0.0631\n",
      "Epoch 1024 | Loss: 0.083595 | LR: 5.7615e-04 | Grad Norm: 0.1339\n",
      "Epoch 1025 | Loss: 0.133742 | LR: 5.7600e-04 | Grad Norm: 0.1885\n",
      "Epoch 1026 | Loss: 0.076583 | LR: 5.7585e-04 | Grad Norm: 0.1826\n",
      "Epoch 1027 | Loss: 0.143258 | LR: 5.7569e-04 | Grad Norm: 0.2258\n",
      "Epoch 1028 | Loss: 0.125636 | LR: 5.7554e-04 | Grad Norm: 0.1511\n",
      "Epoch 1029 | Loss: 0.137284 | LR: 5.7538e-04 | Grad Norm: 0.1648\n",
      "Epoch 1030 | Loss: 0.138736 | LR: 5.7523e-04 | Grad Norm: 0.1853\n",
      "Epoch 1031 | Loss: 0.057091 | LR: 5.7507e-04 | Grad Norm: 0.1332\n",
      "Epoch 1032 | Loss: 0.100497 | LR: 5.7492e-04 | Grad Norm: 0.1507\n",
      "Epoch 1033 | Loss: 0.122328 | LR: 5.7476e-04 | Grad Norm: 0.1910\n",
      "Epoch 1034 | Loss: 0.104846 | LR: 5.7460e-04 | Grad Norm: 0.1357\n",
      "Epoch 1035 | Loss: 0.198085 | LR: 5.7445e-04 | Grad Norm: 0.2200\n",
      "Epoch 1036 | Loss: 0.076741 | LR: 5.7429e-04 | Grad Norm: 0.1283\n",
      "Epoch 1037 | Loss: 0.206293 | LR: 5.7413e-04 | Grad Norm: 0.2315\n",
      "Epoch 1038 | Loss: 0.156911 | LR: 5.7397e-04 | Grad Norm: 0.1955\n",
      "Epoch 1039 | Loss: 0.045579 | LR: 5.7381e-04 | Grad Norm: 0.0638\n",
      "Epoch 1040 | Loss: 0.089217 | LR: 5.7365e-04 | Grad Norm: 0.1256\n",
      "Epoch 1041 | Loss: 0.161745 | LR: 5.7349e-04 | Grad Norm: 0.2267\n",
      "Epoch 1042 | Loss: 0.159916 | LR: 5.7333e-04 | Grad Norm: 0.1950\n",
      "Epoch 1043 | Loss: 0.019235 | LR: 5.7317e-04 | Grad Norm: 0.0608\n",
      "Epoch 1044 | Loss: 0.020205 | LR: 5.7301e-04 | Grad Norm: 0.1183\n",
      "Epoch 1045 | Loss: 0.041228 | LR: 5.7285e-04 | Grad Norm: 0.1160\n",
      "Epoch 1046 | Loss: 0.048288 | LR: 5.7268e-04 | Grad Norm: 0.1179\n",
      "Epoch 1047 | Loss: 0.062758 | LR: 5.7252e-04 | Grad Norm: 0.3050\n",
      "Epoch 1048 | Loss: 0.064938 | LR: 5.7236e-04 | Grad Norm: 0.1254\n",
      "Epoch 1049 | Loss: 0.117267 | LR: 5.7219e-04 | Grad Norm: 0.1798\n",
      "Epoch 1050 | Loss: 0.086505 | LR: 5.7203e-04 | Grad Norm: 0.1200\n",
      "Epoch 1051 | Loss: 0.095905 | LR: 5.7186e-04 | Grad Norm: 0.1752\n",
      "Epoch 1052 | Loss: 0.086512 | LR: 5.7170e-04 | Grad Norm: 0.1382\n",
      "Epoch 1053 | Loss: 0.026451 | LR: 5.7153e-04 | Grad Norm: 0.0549\n",
      "Epoch 1054 | Loss: 0.057928 | LR: 5.7137e-04 | Grad Norm: 0.1512\n",
      "Epoch 1055 | Loss: 0.102583 | LR: 5.7120e-04 | Grad Norm: 0.1589\n",
      "Epoch 1056 | Loss: 0.078244 | LR: 5.7103e-04 | Grad Norm: 0.1922\n",
      "Epoch 1057 | Loss: 0.122124 | LR: 5.7087e-04 | Grad Norm: 0.1700\n",
      "Epoch 1058 | Loss: 0.128180 | LR: 5.7070e-04 | Grad Norm: 0.2226\n",
      "Epoch 1059 | Loss: 0.129282 | LR: 5.7053e-04 | Grad Norm: 0.2041\n",
      "Epoch 1060 | Loss: 0.071513 | LR: 5.7036e-04 | Grad Norm: 0.1247\n",
      "Epoch 1061 | Loss: 0.093139 | LR: 5.7019e-04 | Grad Norm: 0.1406\n",
      "Epoch 1062 | Loss: 0.083506 | LR: 5.7002e-04 | Grad Norm: 0.1959\n",
      "Epoch 1063 | Loss: 0.095403 | LR: 5.6985e-04 | Grad Norm: 0.1313\n",
      "Epoch 1064 | Loss: 0.105314 | LR: 5.6968e-04 | Grad Norm: 0.2077\n",
      "Epoch 1065 | Loss: 0.124702 | LR: 5.6951e-04 | Grad Norm: 0.2273\n",
      "Epoch 1066 | Loss: 0.035082 | LR: 5.6934e-04 | Grad Norm: 0.0783\n",
      "Epoch 1067 | Loss: 0.087423 | LR: 5.6917e-04 | Grad Norm: 0.1294\n",
      "Epoch 1068 | Loss: 0.102712 | LR: 5.6899e-04 | Grad Norm: 0.1841\n",
      "Epoch 1069 | Loss: 0.017152 | LR: 5.6882e-04 | Grad Norm: 0.0448\n",
      "Epoch 1070 | Loss: 0.119184 | LR: 5.6865e-04 | Grad Norm: 0.1663\n",
      "Epoch 1071 | Loss: 0.025906 | LR: 5.6847e-04 | Grad Norm: 0.0634\n",
      "Epoch 1072 | Loss: 0.150847 | LR: 5.6830e-04 | Grad Norm: 0.1938\n",
      "Epoch 1073 | Loss: 0.140508 | LR: 5.6812e-04 | Grad Norm: 0.2724\n",
      "Epoch 1074 | Loss: 0.112592 | LR: 5.6795e-04 | Grad Norm: 0.1832\n",
      "Epoch 1075 | Loss: 0.031309 | LR: 5.6777e-04 | Grad Norm: 0.1743\n",
      "Epoch 1076 | Loss: 0.023963 | LR: 5.6760e-04 | Grad Norm: 0.0912\n",
      "Epoch 1077 | Loss: 0.137959 | LR: 5.6742e-04 | Grad Norm: 0.1707\n",
      "Epoch 1078 | Loss: 0.078844 | LR: 5.6724e-04 | Grad Norm: 0.2184\n",
      "Epoch 1079 | Loss: 0.135880 | LR: 5.6707e-04 | Grad Norm: 0.2082\n",
      "Epoch 1080 | Loss: 0.062070 | LR: 5.6689e-04 | Grad Norm: 0.1530\n",
      "Epoch 1081 | Loss: 0.178054 | LR: 5.6671e-04 | Grad Norm: 0.2241\n",
      "Epoch 1082 | Loss: 0.048741 | LR: 5.6653e-04 | Grad Norm: 0.1484\n",
      "Epoch 1083 | Loss: 0.122038 | LR: 5.6635e-04 | Grad Norm: 0.2478\n",
      "Epoch 1084 | Loss: 0.015255 | LR: 5.6617e-04 | Grad Norm: 0.1739\n",
      "Epoch 1085 | Loss: 0.017623 | LR: 5.6599e-04 | Grad Norm: 0.0787\n",
      "Epoch 1086 | Loss: 0.009973 | LR: 5.6581e-04 | Grad Norm: 0.0416\n",
      "Epoch 1087 | Loss: 0.132020 | LR: 5.6563e-04 | Grad Norm: 0.2203\n",
      "Epoch 1088 | Loss: 0.082208 | LR: 5.6545e-04 | Grad Norm: 0.1606\n",
      "Epoch 1089 | Loss: 0.096141 | LR: 5.6527e-04 | Grad Norm: 0.1495\n",
      "Epoch 1090 | Loss: 0.141662 | LR: 5.6508e-04 | Grad Norm: 0.2055\n",
      "Epoch 1091 | Loss: 0.013849 | LR: 5.6490e-04 | Grad Norm: 0.0622\n",
      "Epoch 1092 | Loss: 0.019052 | LR: 5.6472e-04 | Grad Norm: 0.0630\n",
      "Epoch 1093 | Loss: 0.013374 | LR: 5.6454e-04 | Grad Norm: 0.1106\n",
      "Epoch 1094 | Loss: 0.011428 | LR: 5.6435e-04 | Grad Norm: 0.0836\n",
      "Epoch 1095 | Loss: 0.014273 | LR: 5.6417e-04 | Grad Norm: 0.1411\n",
      "Epoch 1096 | Loss: 0.012608 | LR: 5.6398e-04 | Grad Norm: 0.0446\n",
      "Epoch 1097 | Loss: 0.175451 | LR: 5.6380e-04 | Grad Norm: 0.2546\n",
      "Epoch 1098 | Loss: 0.140166 | LR: 5.6361e-04 | Grad Norm: 0.2209\n",
      "Epoch 1099 | Loss: 0.170027 | LR: 5.6342e-04 | Grad Norm: 0.2104\n",
      "Epoch 1100 | Loss: 0.010794 | LR: 5.6324e-04 | Grad Norm: 0.0598\n",
      "Epoch 1101 | Loss: 0.143115 | LR: 5.6305e-04 | Grad Norm: 0.2468\n",
      "Epoch 1102 | Loss: 0.133709 | LR: 5.6286e-04 | Grad Norm: 0.2359\n",
      "Epoch 1103 | Loss: 0.058879 | LR: 5.6267e-04 | Grad Norm: 0.1457\n",
      "Epoch 1104 | Loss: 0.082832 | LR: 5.6249e-04 | Grad Norm: 0.1417\n",
      "Epoch 1105 | Loss: 0.176089 | LR: 5.6230e-04 | Grad Norm: 0.5859\n",
      "Epoch 1106 | Loss: 0.201115 | LR: 5.6211e-04 | Grad Norm: 0.3373\n",
      "Epoch 1107 | Loss: 0.191339 | LR: 5.6192e-04 | Grad Norm: 0.2394\n",
      "Epoch 1108 | Loss: 0.012544 | LR: 5.6173e-04 | Grad Norm: 0.0729\n",
      "Epoch 1109 | Loss: 0.143651 | LR: 5.6154e-04 | Grad Norm: 0.1867\n",
      "Epoch 1110 | Loss: 0.111649 | LR: 5.6134e-04 | Grad Norm: 0.2042\n",
      "Epoch 1111 | Loss: 0.012742 | LR: 5.6115e-04 | Grad Norm: 0.0484\n",
      "Epoch 1112 | Loss: 0.078254 | LR: 5.6096e-04 | Grad Norm: 0.1456\n",
      "Epoch 1113 | Loss: 0.112571 | LR: 5.6077e-04 | Grad Norm: 0.2346\n",
      "Epoch 1114 | Loss: 0.184176 | LR: 5.6058e-04 | Grad Norm: 0.3586\n",
      "Epoch 1115 | Loss: 0.086904 | LR: 5.6038e-04 | Grad Norm: 0.1472\n",
      "Epoch 1116 | Loss: 0.103999 | LR: 5.6019e-04 | Grad Norm: 1.0117\n",
      "Epoch 1117 | Loss: 0.128523 | LR: 5.5999e-04 | Grad Norm: 0.3786\n",
      "Epoch 1118 | Loss: 0.118868 | LR: 5.5980e-04 | Grad Norm: 0.1850\n",
      "Epoch 1119 | Loss: 0.136626 | LR: 5.5960e-04 | Grad Norm: 0.3456\n",
      "Epoch 1120 | Loss: 0.012283 | LR: 5.5941e-04 | Grad Norm: 0.0717\n",
      "Epoch 1121 | Loss: 0.074343 | LR: 5.5921e-04 | Grad Norm: 0.1813\n",
      "Epoch 1122 | Loss: 0.076593 | LR: 5.5902e-04 | Grad Norm: 0.1349\n",
      "Epoch 1123 | Loss: 0.141880 | LR: 5.5882e-04 | Grad Norm: 0.1919\n",
      "Epoch 1124 | Loss: 0.106928 | LR: 5.5862e-04 | Grad Norm: 0.1994\n",
      "Epoch 1125 | Loss: 0.127827 | LR: 5.5842e-04 | Grad Norm: 0.2512\n",
      "Epoch 1126 | Loss: 0.143226 | LR: 5.5823e-04 | Grad Norm: 0.2297\n",
      "Epoch 1127 | Loss: 0.092885 | LR: 5.5803e-04 | Grad Norm: 0.2757\n",
      "Epoch 1128 | Loss: 0.154151 | LR: 5.5783e-04 | Grad Norm: 0.4935\n",
      "Epoch 1129 | Loss: 0.086834 | LR: 5.5763e-04 | Grad Norm: 0.1655\n",
      "Epoch 1130 | Loss: 0.090326 | LR: 5.5743e-04 | Grad Norm: 0.2449\n",
      "Epoch 1131 | Loss: 0.087557 | LR: 5.5723e-04 | Grad Norm: 0.1745\n",
      "Epoch 1132 | Loss: 0.127995 | LR: 5.5703e-04 | Grad Norm: 0.2270\n",
      "Epoch 1133 | Loss: 0.174314 | LR: 5.5683e-04 | Grad Norm: 0.2275\n",
      "Epoch 1134 | Loss: 0.124830 | LR: 5.5663e-04 | Grad Norm: 0.1673\n",
      "Epoch 1135 | Loss: 0.109857 | LR: 5.5642e-04 | Grad Norm: 0.1750\n",
      "Epoch 1136 | Loss: 0.081044 | LR: 5.5622e-04 | Grad Norm: 0.1638\n",
      "Epoch 1137 | Loss: 0.111437 | LR: 5.5602e-04 | Grad Norm: 0.1583\n",
      "Epoch 1138 | Loss: 0.055200 | LR: 5.5582e-04 | Grad Norm: 0.5003\n",
      "Epoch 1139 | Loss: 0.102382 | LR: 5.5561e-04 | Grad Norm: 0.1711\n",
      "Epoch 1140 | Loss: 0.053086 | LR: 5.5541e-04 | Grad Norm: 0.1578\n",
      "Epoch 1141 | Loss: 0.131550 | LR: 5.5520e-04 | Grad Norm: 0.1815\n",
      "Epoch 1142 | Loss: 0.043486 | LR: 5.5500e-04 | Grad Norm: 0.2045\n",
      "Epoch 1143 | Loss: 0.113317 | LR: 5.5479e-04 | Grad Norm: 0.1883\n",
      "Epoch 1144 | Loss: 0.038663 | LR: 5.5459e-04 | Grad Norm: 0.0685\n",
      "Epoch 1145 | Loss: 0.020121 | LR: 5.5438e-04 | Grad Norm: 0.0682\n",
      "Epoch 1146 | Loss: 0.034898 | LR: 5.5417e-04 | Grad Norm: 0.0821\n",
      "Epoch 1147 | Loss: 0.131391 | LR: 5.5397e-04 | Grad Norm: 0.1679\n",
      "Epoch 1148 | Loss: 0.090942 | LR: 5.5376e-04 | Grad Norm: 0.1333\n",
      "Epoch 1149 | Loss: 0.104327 | LR: 5.5355e-04 | Grad Norm: 0.1795\n",
      "Epoch 1150 | Loss: 0.113616 | LR: 5.5334e-04 | Grad Norm: 0.1604\n",
      "Epoch 1151 | Loss: 0.030907 | LR: 5.5313e-04 | Grad Norm: 0.0854\n",
      "Epoch 1152 | Loss: 0.047177 | LR: 5.5293e-04 | Grad Norm: 0.0834\n",
      "Epoch 1153 | Loss: 0.034797 | LR: 5.5272e-04 | Grad Norm: 0.0676\n",
      "Epoch 1154 | Loss: 0.139041 | LR: 5.5251e-04 | Grad Norm: 0.3270\n",
      "Epoch 1155 | Loss: 0.021320 | LR: 5.5229e-04 | Grad Norm: 0.1190\n",
      "Epoch 1156 | Loss: 0.059406 | LR: 5.5208e-04 | Grad Norm: 0.0803\n",
      "Epoch 1157 | Loss: 0.167436 | LR: 5.5187e-04 | Grad Norm: 0.1763\n",
      "Epoch 1158 | Loss: 0.116984 | LR: 5.5166e-04 | Grad Norm: 0.1753\n",
      "Epoch 1159 | Loss: 0.150176 | LR: 5.5145e-04 | Grad Norm: 0.1885\n",
      "Epoch 1160 | Loss: 0.046283 | LR: 5.5124e-04 | Grad Norm: 0.0954\n",
      "Epoch 1161 | Loss: 0.140164 | LR: 5.5102e-04 | Grad Norm: 0.2067\n",
      "Epoch 1162 | Loss: 0.118048 | LR: 5.5081e-04 | Grad Norm: 0.3790\n",
      "Epoch 1163 | Loss: 0.054992 | LR: 5.5060e-04 | Grad Norm: 0.1594\n",
      "Epoch 1164 | Loss: 0.072502 | LR: 5.5038e-04 | Grad Norm: 0.1467\n",
      "Epoch 1165 | Loss: 0.154513 | LR: 5.5017e-04 | Grad Norm: 0.1923\n",
      "Epoch 1166 | Loss: 0.155280 | LR: 5.4995e-04 | Grad Norm: 0.2075\n",
      "Epoch 1167 | Loss: 0.144501 | LR: 5.4974e-04 | Grad Norm: 0.2281\n",
      "Epoch 1168 | Loss: 0.026865 | LR: 5.4952e-04 | Grad Norm: 0.0945\n",
      "Epoch 1169 | Loss: 0.121765 | LR: 5.4930e-04 | Grad Norm: 0.1700\n",
      "Epoch 1170 | Loss: 0.105115 | LR: 5.4909e-04 | Grad Norm: 0.1536\n",
      "Epoch 1171 | Loss: 0.021697 | LR: 5.4887e-04 | Grad Norm: 0.0823\n",
      "Epoch 1172 | Loss: 0.063973 | LR: 5.4865e-04 | Grad Norm: 0.1253\n",
      "Epoch 1173 | Loss: 0.090214 | LR: 5.4843e-04 | Grad Norm: 0.1473\n",
      "Epoch 1174 | Loss: 0.164906 | LR: 5.4822e-04 | Grad Norm: 0.2279\n",
      "Epoch 1175 | Loss: 0.084783 | LR: 5.4800e-04 | Grad Norm: 0.1697\n",
      "Epoch 1176 | Loss: 0.091757 | LR: 5.4778e-04 | Grad Norm: 0.1422\n",
      "Epoch 1177 | Loss: 0.121009 | LR: 5.4756e-04 | Grad Norm: 0.1642\n",
      "Epoch 1178 | Loss: 0.119923 | LR: 5.4734e-04 | Grad Norm: 0.1622\n",
      "Epoch 1179 | Loss: 0.117589 | LR: 5.4712e-04 | Grad Norm: 0.2621\n",
      "Epoch 1180 | Loss: 0.025618 | LR: 5.4690e-04 | Grad Norm: 0.1634\n",
      "Epoch 1181 | Loss: 0.072934 | LR: 5.4668e-04 | Grad Norm: 0.1427\n",
      "Epoch 1182 | Loss: 0.073363 | LR: 5.4645e-04 | Grad Norm: 0.1281\n",
      "Epoch 1183 | Loss: 0.122358 | LR: 5.4623e-04 | Grad Norm: 0.1474\n",
      "Epoch 1184 | Loss: 0.100121 | LR: 5.4601e-04 | Grad Norm: 0.1686\n",
      "Epoch 1185 | Loss: 0.103510 | LR: 5.4579e-04 | Grad Norm: 0.2467\n",
      "Epoch 1186 | Loss: 0.123404 | LR: 5.4556e-04 | Grad Norm: 0.1840\n",
      "Epoch 1187 | Loss: 0.082501 | LR: 5.4534e-04 | Grad Norm: 0.1668\n",
      "Epoch 1188 | Loss: 0.125013 | LR: 5.4512e-04 | Grad Norm: 0.1619\n",
      "Epoch 1189 | Loss: 0.083792 | LR: 5.4489e-04 | Grad Norm: 0.1714\n",
      "Epoch 1190 | Loss: 0.077805 | LR: 5.4467e-04 | Grad Norm: 0.1763\n",
      "Epoch 1191 | Loss: 0.104024 | LR: 5.4444e-04 | Grad Norm: 0.1389\n",
      "Epoch 1192 | Loss: 0.124574 | LR: 5.4421e-04 | Grad Norm: 0.2086\n",
      "Epoch 1193 | Loss: 0.148350 | LR: 5.4399e-04 | Grad Norm: 0.2345\n",
      "Epoch 1194 | Loss: 0.102058 | LR: 5.4376e-04 | Grad Norm: 0.2001\n",
      "Epoch 1195 | Loss: 0.088256 | LR: 5.4354e-04 | Grad Norm: 0.1933\n",
      "Epoch 1196 | Loss: 0.012296 | LR: 5.4331e-04 | Grad Norm: 0.0842\n",
      "Epoch 1197 | Loss: 0.013383 | LR: 5.4308e-04 | Grad Norm: 0.0434\n",
      "Epoch 1198 | Loss: 0.012035 | LR: 5.4285e-04 | Grad Norm: 0.0902\n",
      "Epoch 1199 | Loss: 0.011490 | LR: 5.4262e-04 | Grad Norm: 0.0528\n",
      "Epoch 1200 | Loss: 0.010323 | LR: 5.4239e-04 | Grad Norm: 0.0597\n",
      "Epoch 1201 | Loss: 0.012502 | LR: 5.4216e-04 | Grad Norm: 0.0541\n",
      "Epoch 1202 | Loss: 0.011152 | LR: 5.4193e-04 | Grad Norm: 0.0699\n",
      "Epoch 1203 | Loss: 0.100871 | LR: 5.4170e-04 | Grad Norm: 0.2116\n",
      "Epoch 1204 | Loss: 0.144235 | LR: 5.4147e-04 | Grad Norm: 0.3307\n",
      "Epoch 1205 | Loss: 0.069991 | LR: 5.4124e-04 | Grad Norm: 0.1533\n",
      "Epoch 1206 | Loss: 0.156499 | LR: 5.4101e-04 | Grad Norm: 0.5797\n",
      "Epoch 1207 | Loss: 0.144663 | LR: 5.4078e-04 | Grad Norm: 0.2063\n",
      "Epoch 1208 | Loss: 0.171980 | LR: 5.4055e-04 | Grad Norm: 0.3045\n",
      "Epoch 1209 | Loss: 0.131718 | LR: 5.4032e-04 | Grad Norm: 0.2211\n",
      "Epoch 1210 | Loss: 0.064723 | LR: 5.4008e-04 | Grad Norm: 0.1470\n",
      "Epoch 1211 | Loss: 0.115697 | LR: 5.3985e-04 | Grad Norm: 0.1860\n",
      "Epoch 1212 | Loss: 0.139136 | LR: 5.3962e-04 | Grad Norm: 0.2103\n",
      "Epoch 1213 | Loss: 0.123147 | LR: 5.3938e-04 | Grad Norm: 0.1823\n",
      "Epoch 1214 | Loss: 0.306547 | LR: 5.3915e-04 | Grad Norm: 0.3466\n",
      "Epoch 1215 | Loss: 0.085485 | LR: 5.3891e-04 | Grad Norm: 0.1581\n",
      "Epoch 1216 | Loss: 0.200591 | LR: 5.3868e-04 | Grad Norm: 0.3757\n",
      "Epoch 1217 | Loss: 0.159936 | LR: 5.3844e-04 | Grad Norm: 0.2534\n",
      "Epoch 1218 | Loss: 0.010743 | LR: 5.3820e-04 | Grad Norm: 0.0914\n",
      "Epoch 1219 | Loss: 0.109645 | LR: 5.3797e-04 | Grad Norm: 0.2245\n",
      "Epoch 1220 | Loss: 0.203001 | LR: 5.3773e-04 | Grad Norm: 0.2921\n",
      "Epoch 1221 | Loss: 0.163628 | LR: 5.3749e-04 | Grad Norm: 0.3432\n",
      "Epoch 1222 | Loss: 0.011784 | LR: 5.3726e-04 | Grad Norm: 0.0477\n",
      "Epoch 1223 | Loss: 0.011031 | LR: 5.3702e-04 | Grad Norm: 0.0594\n",
      "Epoch 1224 | Loss: 0.012103 | LR: 5.3678e-04 | Grad Norm: 0.0481\n",
      "Epoch 1225 | Loss: 0.009831 | LR: 5.3654e-04 | Grad Norm: 0.0310\n",
      "Epoch 1226 | Loss: 0.022821 | LR: 5.3630e-04 | Grad Norm: 0.2990\n",
      "Epoch 1227 | Loss: 0.069317 | LR: 5.3606e-04 | Grad Norm: 0.1465\n",
      "Epoch 1228 | Loss: 0.106382 | LR: 5.3582e-04 | Grad Norm: 0.1972\n",
      "Epoch 1229 | Loss: 0.087397 | LR: 5.3558e-04 | Grad Norm: 0.3895\n",
      "Epoch 1230 | Loss: 0.114769 | LR: 5.3534e-04 | Grad Norm: 0.1766\n",
      "Epoch 1231 | Loss: 0.087816 | LR: 5.3510e-04 | Grad Norm: 0.1604\n",
      "Epoch 1232 | Loss: 0.011299 | LR: 5.3486e-04 | Grad Norm: 0.0599\n",
      "Epoch 1233 | Loss: 0.016468 | LR: 5.3461e-04 | Grad Norm: 0.1464\n",
      "Epoch 1234 | Loss: 0.097088 | LR: 5.3437e-04 | Grad Norm: 0.1704\n",
      "Epoch 1235 | Loss: 0.071398 | LR: 5.3413e-04 | Grad Norm: 0.1276\n",
      "Epoch 1236 | Loss: 0.121507 | LR: 5.3389e-04 | Grad Norm: 0.2113\n",
      "Epoch 1237 | Loss: 0.119429 | LR: 5.3364e-04 | Grad Norm: 0.2217\n",
      "Epoch 1238 | Loss: 0.117462 | LR: 5.3340e-04 | Grad Norm: 0.2139\n",
      "Epoch 1239 | Loss: 0.079206 | LR: 5.3315e-04 | Grad Norm: 0.1726\n",
      "Epoch 1240 | Loss: 0.082389 | LR: 5.3291e-04 | Grad Norm: 0.1975\n",
      "Epoch 1241 | Loss: 0.078636 | LR: 5.3266e-04 | Grad Norm: 0.1584\n",
      "Epoch 1242 | Loss: 0.080479 | LR: 5.3242e-04 | Grad Norm: 0.1903\n",
      "Epoch 1243 | Loss: 0.103938 | LR: 5.3217e-04 | Grad Norm: 0.2326\n",
      "Epoch 1244 | Loss: 0.107137 | LR: 5.3193e-04 | Grad Norm: 0.1863\n",
      "Epoch 1245 | Loss: 0.010901 | LR: 5.3168e-04 | Grad Norm: 0.0474\n",
      "Epoch 1246 | Loss: 0.100142 | LR: 5.3143e-04 | Grad Norm: 0.1902\n",
      "Epoch 1247 | Loss: 0.077710 | LR: 5.3119e-04 | Grad Norm: 0.2537\n",
      "Epoch 1248 | Loss: 0.011130 | LR: 5.3094e-04 | Grad Norm: 0.0621\n",
      "Epoch 1249 | Loss: 0.121988 | LR: 5.3069e-04 | Grad Norm: 0.2750\n",
      "Epoch 1250 | Loss: 0.011474 | LR: 5.3044e-04 | Grad Norm: 0.0345\n",
      "Epoch 1251 | Loss: 0.146534 | LR: 5.3019e-04 | Grad Norm: 0.2811\n",
      "Epoch 1252 | Loss: 0.112882 | LR: 5.2994e-04 | Grad Norm: 0.2123\n",
      "Epoch 1253 | Loss: 0.112521 | LR: 5.2969e-04 | Grad Norm: 0.2422\n",
      "Epoch 1254 | Loss: 0.014915 | LR: 5.2944e-04 | Grad Norm: 0.1861\n",
      "Epoch 1255 | Loss: 0.011203 | LR: 5.2919e-04 | Grad Norm: 0.0456\n",
      "Epoch 1256 | Loss: 0.026430 | LR: 5.2894e-04 | Grad Norm: 0.0892\n",
      "Epoch 1257 | Loss: 0.016072 | LR: 5.2869e-04 | Grad Norm: 0.0494\n",
      "Epoch 1258 | Loss: 0.025395 | LR: 5.2844e-04 | Grad Norm: 0.0551\n",
      "Epoch 1259 | Loss: 0.013651 | LR: 5.2819e-04 | Grad Norm: 0.0421\n",
      "Epoch 1260 | Loss: 0.049378 | LR: 5.2794e-04 | Grad Norm: 0.1132\n",
      "Epoch 1261 | Loss: 0.014086 | LR: 5.2768e-04 | Grad Norm: 0.0525\n",
      "Epoch 1262 | Loss: 0.031245 | LR: 5.2743e-04 | Grad Norm: 0.0912\n",
      "Epoch 1263 | Loss: 0.070231 | LR: 5.2718e-04 | Grad Norm: 0.1623\n",
      "Epoch 1264 | Loss: 0.109301 | LR: 5.2692e-04 | Grad Norm: 0.2061\n",
      "Epoch 1265 | Loss: 0.070243 | LR: 5.2667e-04 | Grad Norm: 0.2120\n",
      "Epoch 1266 | Loss: 0.130700 | LR: 5.2641e-04 | Grad Norm: 0.2021\n",
      "Epoch 1267 | Loss: 0.119008 | LR: 5.2616e-04 | Grad Norm: 0.1519\n",
      "Epoch 1268 | Loss: 0.130873 | LR: 5.2590e-04 | Grad Norm: 0.2151\n",
      "Epoch 1269 | Loss: 0.128741 | LR: 5.2565e-04 | Grad Norm: 0.1854\n",
      "Epoch 1270 | Loss: 0.055091 | LR: 5.2539e-04 | Grad Norm: 0.1328\n",
      "Epoch 1271 | Loss: 0.096940 | LR: 5.2514e-04 | Grad Norm: 0.1623\n",
      "Epoch 1272 | Loss: 0.111391 | LR: 5.2488e-04 | Grad Norm: 0.1686\n",
      "Epoch 1273 | Loss: 0.100020 | LR: 5.2462e-04 | Grad Norm: 0.1881\n",
      "Epoch 1274 | Loss: 0.200782 | LR: 5.2436e-04 | Grad Norm: 0.2290\n",
      "Epoch 1275 | Loss: 0.070991 | LR: 5.2411e-04 | Grad Norm: 0.1354\n",
      "Epoch 1276 | Loss: 0.182483 | LR: 5.2385e-04 | Grad Norm: 0.2716\n",
      "Epoch 1277 | Loss: 0.140085 | LR: 5.2359e-04 | Grad Norm: 0.1914\n",
      "Epoch 1278 | Loss: 0.041207 | LR: 5.2333e-04 | Grad Norm: 0.0595\n",
      "Epoch 1279 | Loss: 0.086871 | LR: 5.2307e-04 | Grad Norm: 0.1432\n",
      "Epoch 1280 | Loss: 0.149580 | LR: 5.2281e-04 | Grad Norm: 0.1784\n",
      "Epoch 1281 | Loss: 0.141313 | LR: 5.2255e-04 | Grad Norm: 0.2019\n",
      "Epoch 1282 | Loss: 0.016049 | LR: 5.2229e-04 | Grad Norm: 0.0866\n",
      "Epoch 1283 | Loss: 0.018744 | LR: 5.2203e-04 | Grad Norm: 0.1014\n",
      "Epoch 1284 | Loss: 0.040291 | LR: 5.2177e-04 | Grad Norm: 0.1014\n",
      "Epoch 1285 | Loss: 0.042133 | LR: 5.2151e-04 | Grad Norm: 0.0755\n",
      "Epoch 1286 | Loss: 0.051365 | LR: 5.2125e-04 | Grad Norm: 0.2197\n",
      "Epoch 1287 | Loss: 0.059030 | LR: 5.2098e-04 | Grad Norm: 0.1217\n",
      "Epoch 1288 | Loss: 0.102009 | LR: 5.2072e-04 | Grad Norm: 0.2009\n",
      "Epoch 1289 | Loss: 0.082356 | LR: 5.2046e-04 | Grad Norm: 0.2328\n",
      "Epoch 1290 | Loss: 0.086600 | LR: 5.2020e-04 | Grad Norm: 0.1691\n",
      "Epoch 1291 | Loss: 0.083338 | LR: 5.1993e-04 | Grad Norm: 0.2787\n",
      "Epoch 1292 | Loss: 0.022518 | LR: 5.1967e-04 | Grad Norm: 0.1033\n",
      "Epoch 1293 | Loss: 0.053404 | LR: 5.1940e-04 | Grad Norm: 0.1323\n",
      "Epoch 1294 | Loss: 0.084520 | LR: 5.1914e-04 | Grad Norm: 0.1401\n",
      "Epoch 1295 | Loss: 0.070192 | LR: 5.1887e-04 | Grad Norm: 0.1198\n",
      "Epoch 1296 | Loss: 0.107714 | LR: 5.1861e-04 | Grad Norm: 0.1672\n",
      "Epoch 1297 | Loss: 0.113009 | LR: 5.1834e-04 | Grad Norm: 0.1987\n",
      "Epoch 1298 | Loss: 0.109015 | LR: 5.1808e-04 | Grad Norm: 0.1906\n",
      "Epoch 1299 | Loss: 0.067713 | LR: 5.1781e-04 | Grad Norm: 0.2020\n",
      "Epoch 1300 | Loss: 0.073600 | LR: 5.1754e-04 | Grad Norm: 0.1534\n",
      "Epoch 1301 | Loss: 0.073611 | LR: 5.1728e-04 | Grad Norm: 0.1539\n",
      "Epoch 1302 | Loss: 0.093223 | LR: 5.1701e-04 | Grad Norm: 0.1632\n",
      "Epoch 1303 | Loss: 0.101220 | LR: 5.1674e-04 | Grad Norm: 0.3159\n",
      "Epoch 1304 | Loss: 0.102645 | LR: 5.1647e-04 | Grad Norm: 0.1688\n",
      "Epoch 1305 | Loss: 0.042311 | LR: 5.1620e-04 | Grad Norm: 0.1653\n",
      "Epoch 1306 | Loss: 0.082353 | LR: 5.1593e-04 | Grad Norm: 0.1470\n",
      "Epoch 1307 | Loss: 0.091034 | LR: 5.1566e-04 | Grad Norm: 0.1875\n",
      "Epoch 1308 | Loss: 0.015645 | LR: 5.1540e-04 | Grad Norm: 0.0689\n",
      "Epoch 1309 | Loss: 0.112504 | LR: 5.1513e-04 | Grad Norm: 0.1754\n",
      "Epoch 1310 | Loss: 0.023929 | LR: 5.1485e-04 | Grad Norm: 0.1053\n",
      "Epoch 1311 | Loss: 0.139232 | LR: 5.1458e-04 | Grad Norm: 0.2583\n",
      "Epoch 1312 | Loss: 0.134012 | LR: 5.1431e-04 | Grad Norm: 0.2251\n",
      "Epoch 1313 | Loss: 0.108052 | LR: 5.1404e-04 | Grad Norm: 0.1949\n",
      "Epoch 1314 | Loss: 0.029097 | LR: 5.1377e-04 | Grad Norm: 0.1667\n",
      "Epoch 1315 | Loss: 0.019731 | LR: 5.1350e-04 | Grad Norm: 0.0604\n",
      "Epoch 1316 | Loss: 0.122468 | LR: 5.1323e-04 | Grad Norm: 0.2244\n",
      "Epoch 1317 | Loss: 0.051438 | LR: 5.1295e-04 | Grad Norm: 0.2183\n",
      "Epoch 1318 | Loss: 0.121110 | LR: 5.1268e-04 | Grad Norm: 0.2036\n",
      "Epoch 1319 | Loss: 0.049836 | LR: 5.1241e-04 | Grad Norm: 0.1842\n",
      "Epoch 1320 | Loss: 0.178671 | LR: 5.1213e-04 | Grad Norm: 0.2617\n",
      "Epoch 1321 | Loss: 0.041514 | LR: 5.1186e-04 | Grad Norm: 0.1441\n",
      "Epoch 1322 | Loss: 0.128417 | LR: 5.1158e-04 | Grad Norm: 0.2182\n",
      "Epoch 1323 | Loss: 0.011417 | LR: 5.1131e-04 | Grad Norm: 0.0834\n",
      "Epoch 1324 | Loss: 0.013580 | LR: 5.1103e-04 | Grad Norm: 0.0608\n",
      "Epoch 1325 | Loss: 0.008288 | LR: 5.1076e-04 | Grad Norm: 0.0424\n",
      "Epoch 1326 | Loss: 0.106632 | LR: 5.1048e-04 | Grad Norm: 0.1782\n",
      "Epoch 1327 | Loss: 0.068876 | LR: 5.1021e-04 | Grad Norm: 0.1565\n",
      "Epoch 1328 | Loss: 0.090128 | LR: 5.0993e-04 | Grad Norm: 0.1684\n",
      "Epoch 1329 | Loss: 0.132829 | LR: 5.0965e-04 | Grad Norm: 0.2423\n",
      "Epoch 1330 | Loss: 0.010659 | LR: 5.0937e-04 | Grad Norm: 0.0851\n",
      "Epoch 1331 | Loss: 0.019571 | LR: 5.0910e-04 | Grad Norm: 0.0816\n",
      "Epoch 1332 | Loss: 0.010087 | LR: 5.0882e-04 | Grad Norm: 0.0331\n",
      "Epoch 1333 | Loss: 0.011651 | LR: 5.0854e-04 | Grad Norm: 0.0925\n",
      "Epoch 1334 | Loss: 0.013484 | LR: 5.0826e-04 | Grad Norm: 0.1622\n",
      "Epoch 1335 | Loss: 0.011100 | LR: 5.0798e-04 | Grad Norm: 0.0590\n",
      "Epoch 1336 | Loss: 0.166636 | LR: 5.0770e-04 | Grad Norm: 0.2289\n",
      "Epoch 1337 | Loss: 0.130441 | LR: 5.0742e-04 | Grad Norm: 0.2609\n",
      "Epoch 1338 | Loss: 0.161098 | LR: 5.0714e-04 | Grad Norm: 0.3434\n",
      "Epoch 1339 | Loss: 0.010077 | LR: 5.0686e-04 | Grad Norm: 0.0325\n",
      "Epoch 1340 | Loss: 0.136360 | LR: 5.0658e-04 | Grad Norm: 0.3183\n",
      "Epoch 1341 | Loss: 0.114905 | LR: 5.0630e-04 | Grad Norm: 0.3077\n",
      "Epoch 1342 | Loss: 0.053801 | LR: 5.0602e-04 | Grad Norm: 0.1727\n",
      "Epoch 1343 | Loss: 0.064088 | LR: 5.0574e-04 | Grad Norm: 0.1345\n",
      "Epoch 1344 | Loss: 0.160426 | LR: 5.0546e-04 | Grad Norm: 0.2439\n",
      "Epoch 1345 | Loss: 0.192245 | LR: 5.0517e-04 | Grad Norm: 0.4182\n",
      "Epoch 1346 | Loss: 0.169750 | LR: 5.0489e-04 | Grad Norm: 0.4905\n",
      "Epoch 1347 | Loss: 0.011313 | LR: 5.0461e-04 | Grad Norm: 0.0822\n",
      "Epoch 1348 | Loss: 0.132061 | LR: 5.0433e-04 | Grad Norm: 0.2663\n",
      "Epoch 1349 | Loss: 0.112854 | LR: 5.0404e-04 | Grad Norm: 0.4488\n",
      "Epoch 1350 | Loss: 0.013518 | LR: 5.0376e-04 | Grad Norm: 0.0778\n",
      "Epoch 1351 | Loss: 0.068649 | LR: 5.0347e-04 | Grad Norm: 0.1515\n",
      "Epoch 1352 | Loss: 0.104840 | LR: 5.0319e-04 | Grad Norm: 0.2190\n",
      "Epoch 1353 | Loss: 0.195065 | LR: 5.0290e-04 | Grad Norm: 1.0169\n",
      "Epoch 1354 | Loss: 0.089343 | LR: 5.0262e-04 | Grad Norm: 0.1577\n",
      "Epoch 1355 | Loss: 0.098386 | LR: 5.0233e-04 | Grad Norm: 0.3622\n",
      "Epoch 1356 | Loss: 0.108881 | LR: 5.0205e-04 | Grad Norm: 0.2146\n",
      "Epoch 1357 | Loss: 0.112647 | LR: 5.0176e-04 | Grad Norm: 0.1963\n",
      "Epoch 1358 | Loss: 0.127983 | LR: 5.0147e-04 | Grad Norm: 0.3345\n",
      "Epoch 1359 | Loss: 0.010366 | LR: 5.0119e-04 | Grad Norm: 0.0324\n",
      "Epoch 1360 | Loss: 0.067766 | LR: 5.0090e-04 | Grad Norm: 0.1637\n",
      "Epoch 1361 | Loss: 0.075801 | LR: 5.0061e-04 | Grad Norm: 0.1725\n",
      "Epoch 1362 | Loss: 0.127279 | LR: 5.0032e-04 | Grad Norm: 0.2173\n",
      "Epoch 1363 | Loss: 0.102310 | LR: 5.0004e-04 | Grad Norm: 0.1876\n",
      "Epoch 1364 | Loss: 0.110892 | LR: 4.9975e-04 | Grad Norm: 0.2639\n",
      "Epoch 1365 | Loss: 0.122334 | LR: 4.9946e-04 | Grad Norm: 0.1931\n",
      "Epoch 1366 | Loss: 0.064533 | LR: 4.9917e-04 | Grad Norm: 0.1591\n",
      "Epoch 1367 | Loss: 0.130420 | LR: 4.9888e-04 | Grad Norm: 0.2119\n",
      "Epoch 1368 | Loss: 0.060959 | LR: 4.9859e-04 | Grad Norm: 0.1923\n",
      "Epoch 1369 | Loss: 0.076179 | LR: 4.9830e-04 | Grad Norm: 0.2454\n",
      "Epoch 1370 | Loss: 0.085057 | LR: 4.9801e-04 | Grad Norm: 0.1730\n",
      "Epoch 1371 | Loss: 0.127895 | LR: 4.9772e-04 | Grad Norm: 0.2218\n",
      "Epoch 1372 | Loss: 0.165226 | LR: 4.9743e-04 | Grad Norm: 0.2346\n",
      "Epoch 1373 | Loss: 0.108894 | LR: 4.9714e-04 | Grad Norm: 0.1895\n",
      "Epoch 1374 | Loss: 0.097623 | LR: 4.9685e-04 | Grad Norm: 0.2223\n",
      "Epoch 1375 | Loss: 0.066336 | LR: 4.9655e-04 | Grad Norm: 0.1398\n",
      "Epoch 1376 | Loss: 0.094577 | LR: 4.9626e-04 | Grad Norm: 0.1607\n",
      "Epoch 1377 | Loss: 0.056577 | LR: 4.9597e-04 | Grad Norm: 0.1976\n",
      "Epoch 1378 | Loss: 0.095720 | LR: 4.9568e-04 | Grad Norm: 0.1597\n",
      "Epoch 1379 | Loss: 0.040782 | LR: 4.9538e-04 | Grad Norm: 0.1314\n",
      "Epoch 1380 | Loss: 0.133444 | LR: 4.9509e-04 | Grad Norm: 0.1917\n",
      "Epoch 1381 | Loss: 0.034395 | LR: 4.9479e-04 | Grad Norm: 0.1270\n",
      "Epoch 1382 | Loss: 0.101020 | LR: 4.9450e-04 | Grad Norm: 0.1564\n",
      "Epoch 1383 | Loss: 0.033273 | LR: 4.9421e-04 | Grad Norm: 0.0611\n",
      "Epoch 1384 | Loss: 0.017845 | LR: 4.9391e-04 | Grad Norm: 0.0518\n",
      "Epoch 1385 | Loss: 0.029563 | LR: 4.9362e-04 | Grad Norm: 0.0718\n",
      "Epoch 1386 | Loss: 0.111887 | LR: 4.9332e-04 | Grad Norm: 0.1951\n",
      "Epoch 1387 | Loss: 0.085060 | LR: 4.9302e-04 | Grad Norm: 0.1528\n",
      "Epoch 1388 | Loss: 0.100495 | LR: 4.9273e-04 | Grad Norm: 0.2060\n",
      "Epoch 1389 | Loss: 0.113633 | LR: 4.9243e-04 | Grad Norm: 0.2650\n",
      "Epoch 1390 | Loss: 0.026427 | LR: 4.9214e-04 | Grad Norm: 0.0828\n",
      "Epoch 1391 | Loss: 0.050668 | LR: 4.9184e-04 | Grad Norm: 0.1408\n",
      "Epoch 1392 | Loss: 0.037376 | LR: 4.9154e-04 | Grad Norm: 0.0778\n",
      "Epoch 1393 | Loss: 0.135734 | LR: 4.9124e-04 | Grad Norm: 0.2784\n",
      "Epoch 1394 | Loss: 0.023158 | LR: 4.9095e-04 | Grad Norm: 0.1768\n",
      "Epoch 1395 | Loss: 0.059469 | LR: 4.9065e-04 | Grad Norm: 0.1145\n",
      "Epoch 1396 | Loss: 0.155096 | LR: 4.9035e-04 | Grad Norm: 0.1797\n",
      "Epoch 1397 | Loss: 0.101162 | LR: 4.9005e-04 | Grad Norm: 0.2727\n",
      "Epoch 1398 | Loss: 0.134249 | LR: 4.8975e-04 | Grad Norm: 0.2090\n",
      "Epoch 1399 | Loss: 0.039720 | LR: 4.8945e-04 | Grad Norm: 0.0863\n",
      "Epoch 1400 | Loss: 0.129908 | LR: 4.8915e-04 | Grad Norm: 0.2154\n",
      "Epoch 1401 | Loss: 0.089646 | LR: 4.8885e-04 | Grad Norm: 0.1678\n",
      "Epoch 1402 | Loss: 0.042823 | LR: 4.8855e-04 | Grad Norm: 0.1506\n",
      "Epoch 1403 | Loss: 0.058710 | LR: 4.8825e-04 | Grad Norm: 0.1891\n",
      "Epoch 1404 | Loss: 0.119901 | LR: 4.8795e-04 | Grad Norm: 0.2306\n",
      "Epoch 1405 | Loss: 0.125390 | LR: 4.8765e-04 | Grad Norm: 0.2015\n",
      "Epoch 1406 | Loss: 0.125767 | LR: 4.8735e-04 | Grad Norm: 0.2607\n",
      "Epoch 1407 | Loss: 0.020910 | LR: 4.8705e-04 | Grad Norm: 0.0612\n",
      "Epoch 1408 | Loss: 0.105582 | LR: 4.8674e-04 | Grad Norm: 0.1602\n",
      "Epoch 1409 | Loss: 0.100050 | LR: 4.8644e-04 | Grad Norm: 0.1850\n",
      "Epoch 1410 | Loss: 0.022716 | LR: 4.8614e-04 | Grad Norm: 0.0895\n",
      "Epoch 1411 | Loss: 0.063759 | LR: 4.8584e-04 | Grad Norm: 0.1299\n",
      "Epoch 1412 | Loss: 0.079034 | LR: 4.8553e-04 | Grad Norm: 0.1512\n",
      "Epoch 1413 | Loss: 0.148624 | LR: 4.8523e-04 | Grad Norm: 0.2453\n",
      "Epoch 1414 | Loss: 0.076244 | LR: 4.8492e-04 | Grad Norm: 0.1471\n",
      "Epoch 1415 | Loss: 0.088541 | LR: 4.8462e-04 | Grad Norm: 0.2924\n",
      "Epoch 1416 | Loss: 0.097027 | LR: 4.8432e-04 | Grad Norm: 0.1657\n",
      "Epoch 1417 | Loss: 0.108521 | LR: 4.8401e-04 | Grad Norm: 0.3809\n",
      "Epoch 1418 | Loss: 0.111363 | LR: 4.8371e-04 | Grad Norm: 0.3242\n",
      "Epoch 1419 | Loss: 0.019665 | LR: 4.8340e-04 | Grad Norm: 0.0835\n",
      "Epoch 1420 | Loss: 0.066838 | LR: 4.8310e-04 | Grad Norm: 0.1480\n",
      "Epoch 1421 | Loss: 0.071620 | LR: 4.8279e-04 | Grad Norm: 0.1598\n",
      "Epoch 1422 | Loss: 0.114968 | LR: 4.8248e-04 | Grad Norm: 0.2128\n",
      "Epoch 1423 | Loss: 0.093721 | LR: 4.8218e-04 | Grad Norm: 0.1491\n",
      "Epoch 1424 | Loss: 0.075976 | LR: 4.8187e-04 | Grad Norm: 0.1789\n",
      "Epoch 1425 | Loss: 0.107478 | LR: 4.8156e-04 | Grad Norm: 0.2428\n",
      "Epoch 1426 | Loss: 0.067302 | LR: 4.8126e-04 | Grad Norm: 0.1452\n",
      "Epoch 1427 | Loss: 0.103032 | LR: 4.8095e-04 | Grad Norm: 0.1693\n",
      "Epoch 1428 | Loss: 0.077835 | LR: 4.8064e-04 | Grad Norm: 0.2587\n",
      "Epoch 1429 | Loss: 0.059248 | LR: 4.8033e-04 | Grad Norm: 0.1361\n",
      "Epoch 1430 | Loss: 0.092603 | LR: 4.8002e-04 | Grad Norm: 0.1608\n",
      "Epoch 1431 | Loss: 0.110186 | LR: 4.7971e-04 | Grad Norm: 0.2201\n",
      "Epoch 1432 | Loss: 0.136209 | LR: 4.7941e-04 | Grad Norm: 0.1831\n",
      "Epoch 1433 | Loss: 0.093527 | LR: 4.7910e-04 | Grad Norm: 0.1685\n",
      "Epoch 1434 | Loss: 0.074200 | LR: 4.7879e-04 | Grad Norm: 0.1721\n",
      "Epoch 1435 | Loss: 0.009569 | LR: 4.7848e-04 | Grad Norm: 0.0714\n",
      "Epoch 1436 | Loss: 0.014585 | LR: 4.7817e-04 | Grad Norm: 0.0756\n",
      "Epoch 1437 | Loss: 0.009600 | LR: 4.7786e-04 | Grad Norm: 0.0898\n",
      "Epoch 1438 | Loss: 0.009338 | LR: 4.7755e-04 | Grad Norm: 0.0517\n",
      "Epoch 1439 | Loss: 0.009017 | LR: 4.7723e-04 | Grad Norm: 0.0435\n",
      "Epoch 1440 | Loss: 0.010152 | LR: 4.7692e-04 | Grad Norm: 0.0303\n",
      "Epoch 1441 | Loss: 0.008732 | LR: 4.7661e-04 | Grad Norm: 0.0359\n",
      "Epoch 1442 | Loss: 0.081222 | LR: 4.7630e-04 | Grad Norm: 0.1931\n",
      "Epoch 1443 | Loss: 0.130978 | LR: 4.7599e-04 | Grad Norm: 0.2966\n",
      "Epoch 1444 | Loss: 0.073500 | LR: 4.7567e-04 | Grad Norm: 0.1836\n",
      "Epoch 1445 | Loss: 0.127662 | LR: 4.7536e-04 | Grad Norm: 0.2217\n",
      "Epoch 1446 | Loss: 0.133998 | LR: 4.7505e-04 | Grad Norm: 0.2621\n",
      "Epoch 1447 | Loss: 0.142598 | LR: 4.7474e-04 | Grad Norm: 0.2518\n",
      "Epoch 1448 | Loss: 0.129574 | LR: 4.7442e-04 | Grad Norm: 0.2497\n",
      "Epoch 1449 | Loss: 0.050914 | LR: 4.7411e-04 | Grad Norm: 0.1767\n",
      "Epoch 1450 | Loss: 0.106201 | LR: 4.7379e-04 | Grad Norm: 0.3437\n",
      "Epoch 1451 | Loss: 0.140006 | LR: 4.7348e-04 | Grad Norm: 0.2231\n",
      "Epoch 1452 | Loss: 0.141042 | LR: 4.7317e-04 | Grad Norm: 0.3559\n",
      "Epoch 1453 | Loss: 0.286797 | LR: 4.7285e-04 | Grad Norm: 0.3455\n",
      "Epoch 1454 | Loss: 0.076956 | LR: 4.7254e-04 | Grad Norm: 0.1614\n",
      "Epoch 1455 | Loss: 0.189324 | LR: 4.7222e-04 | Grad Norm: 0.3481\n",
      "Epoch 1456 | Loss: 0.146584 | LR: 4.7190e-04 | Grad Norm: 0.2373\n",
      "Epoch 1457 | Loss: 0.009940 | LR: 4.7159e-04 | Grad Norm: 0.0847\n",
      "Epoch 1458 | Loss: 0.089902 | LR: 4.7127e-04 | Grad Norm: 0.2185\n",
      "Epoch 1459 | Loss: 0.145529 | LR: 4.7096e-04 | Grad Norm: 0.2450\n",
      "Epoch 1460 | Loss: 0.141133 | LR: 4.7064e-04 | Grad Norm: 0.3982\n",
      "Epoch 1461 | Loss: 0.011899 | LR: 4.7032e-04 | Grad Norm: 0.1047\n",
      "Epoch 1462 | Loss: 0.008392 | LR: 4.7000e-04 | Grad Norm: 0.0421\n",
      "Epoch 1463 | Loss: 0.008458 | LR: 4.6969e-04 | Grad Norm: 0.0448\n",
      "Epoch 1464 | Loss: 0.008949 | LR: 4.6937e-04 | Grad Norm: 0.0462\n",
      "Epoch 1465 | Loss: 0.016679 | LR: 4.6905e-04 | Grad Norm: 0.1773\n",
      "Epoch 1466 | Loss: 0.062649 | LR: 4.6873e-04 | Grad Norm: 0.1525\n",
      "Epoch 1467 | Loss: 0.084365 | LR: 4.6841e-04 | Grad Norm: 0.2016\n",
      "Epoch 1468 | Loss: 0.069215 | LR: 4.6810e-04 | Grad Norm: 0.2522\n",
      "Epoch 1469 | Loss: 0.097104 | LR: 4.6778e-04 | Grad Norm: 0.2216\n",
      "Epoch 1470 | Loss: 0.082561 | LR: 4.6746e-04 | Grad Norm: 0.2402\n",
      "Epoch 1471 | Loss: 0.009071 | LR: 4.6714e-04 | Grad Norm: 0.0253\n",
      "Epoch 1472 | Loss: 0.016823 | LR: 4.6682e-04 | Grad Norm: 0.2149\n",
      "Epoch 1473 | Loss: 0.083767 | LR: 4.6650e-04 | Grad Norm: 0.1763\n",
      "Epoch 1474 | Loss: 0.064236 | LR: 4.6618e-04 | Grad Norm: 0.1394\n",
      "Epoch 1475 | Loss: 0.113647 | LR: 4.6586e-04 | Grad Norm: 0.2533\n",
      "Epoch 1476 | Loss: 0.101460 | LR: 4.6554e-04 | Grad Norm: 0.2259\n",
      "Epoch 1477 | Loss: 0.133187 | LR: 4.6521e-04 | Grad Norm: 0.5699\n",
      "Epoch 1478 | Loss: 0.078620 | LR: 4.6489e-04 | Grad Norm: 0.2500\n",
      "Epoch 1479 | Loss: 0.080900 | LR: 4.6457e-04 | Grad Norm: 0.2302\n",
      "Epoch 1480 | Loss: 0.081046 | LR: 4.6425e-04 | Grad Norm: 0.2070\n",
      "Epoch 1481 | Loss: 0.085152 | LR: 4.6393e-04 | Grad Norm: 0.2065\n",
      "Epoch 1482 | Loss: 0.077879 | LR: 4.6360e-04 | Grad Norm: 0.2330\n",
      "Epoch 1483 | Loss: 0.109849 | LR: 4.6328e-04 | Grad Norm: 0.2186\n",
      "Epoch 1484 | Loss: 0.011054 | LR: 4.6296e-04 | Grad Norm: 0.0605\n",
      "Epoch 1485 | Loss: 0.086884 | LR: 4.6264e-04 | Grad Norm: 0.2594\n",
      "Epoch 1486 | Loss: 0.071812 | LR: 4.6231e-04 | Grad Norm: 0.1661\n",
      "Epoch 1487 | Loss: 0.008672 | LR: 4.6199e-04 | Grad Norm: 0.0327\n",
      "Epoch 1488 | Loss: 0.111552 | LR: 4.6166e-04 | Grad Norm: 0.2095\n",
      "Epoch 1489 | Loss: 0.009859 | LR: 4.6134e-04 | Grad Norm: 0.0315\n",
      "Epoch 1490 | Loss: 0.120646 | LR: 4.6102e-04 | Grad Norm: 0.2500\n",
      "Epoch 1491 | Loss: 0.107502 | LR: 4.6069e-04 | Grad Norm: 0.2010\n",
      "Epoch 1492 | Loss: 0.101125 | LR: 4.6037e-04 | Grad Norm: 0.2129\n",
      "Epoch 1493 | Loss: 0.014995 | LR: 4.6004e-04 | Grad Norm: 0.2097\n",
      "Epoch 1494 | Loss: 0.011620 | LR: 4.5972e-04 | Grad Norm: 0.0502\n",
      "Epoch 1495 | Loss: 0.022144 | LR: 4.5939e-04 | Grad Norm: 0.0767\n",
      "Epoch 1496 | Loss: 0.014151 | LR: 4.5906e-04 | Grad Norm: 0.0502\n",
      "Epoch 1497 | Loss: 0.022707 | LR: 4.5874e-04 | Grad Norm: 0.0705\n",
      "Epoch 1498 | Loss: 0.012018 | LR: 4.5841e-04 | Grad Norm: 0.0354\n",
      "Epoch 1499 | Loss: 0.045351 | LR: 4.5808e-04 | Grad Norm: 0.1340\n",
      "Epoch 1500 | Loss: 0.010300 | LR: 4.5776e-04 | Grad Norm: 0.0410\n",
      "Epoch 1501 | Loss: 0.029054 | LR: 4.5743e-04 | Grad Norm: 0.0677\n",
      "Epoch 1502 | Loss: 0.065795 | LR: 4.5710e-04 | Grad Norm: 0.1413\n",
      "Epoch 1503 | Loss: 0.098806 | LR: 4.5678e-04 | Grad Norm: 0.4516\n",
      "Epoch 1504 | Loss: 0.062100 | LR: 4.5645e-04 | Grad Norm: 0.2314\n",
      "Epoch 1505 | Loss: 0.123108 | LR: 4.5612e-04 | Grad Norm: 0.2701\n",
      "Epoch 1506 | Loss: 0.111615 | LR: 4.5579e-04 | Grad Norm: 0.2216\n",
      "Epoch 1507 | Loss: 0.133162 | LR: 4.5546e-04 | Grad Norm: 0.2314\n",
      "Epoch 1508 | Loss: 0.119378 | LR: 4.5513e-04 | Grad Norm: 0.2026\n",
      "Epoch 1509 | Loss: 0.044785 | LR: 4.5480e-04 | Grad Norm: 0.1364\n",
      "Epoch 1510 | Loss: 0.087209 | LR: 4.5448e-04 | Grad Norm: 0.1656\n",
      "Epoch 1511 | Loss: 0.111689 | LR: 4.5415e-04 | Grad Norm: 0.1878\n",
      "Epoch 1512 | Loss: 0.107791 | LR: 4.5382e-04 | Grad Norm: 0.2456\n",
      "Epoch 1513 | Loss: 0.185345 | LR: 4.5349e-04 | Grad Norm: 0.3184\n",
      "Epoch 1514 | Loss: 0.069073 | LR: 4.5316e-04 | Grad Norm: 0.2010\n",
      "Epoch 1515 | Loss: 0.179005 | LR: 4.5283e-04 | Grad Norm: 0.3425\n",
      "Epoch 1516 | Loss: 0.156829 | LR: 4.5249e-04 | Grad Norm: 0.2093\n",
      "Epoch 1517 | Loss: 0.042659 | LR: 4.5216e-04 | Grad Norm: 0.0781\n",
      "Epoch 1518 | Loss: 0.082506 | LR: 4.5183e-04 | Grad Norm: 0.1694\n",
      "Epoch 1519 | Loss: 0.147641 | LR: 4.5150e-04 | Grad Norm: 0.2100\n",
      "Epoch 1520 | Loss: 0.131491 | LR: 4.5117e-04 | Grad Norm: 0.3912\n",
      "Epoch 1521 | Loss: 0.014278 | LR: 4.5084e-04 | Grad Norm: 0.0554\n",
      "Epoch 1522 | Loss: 0.014035 | LR: 4.5051e-04 | Grad Norm: 0.0351\n",
      "Epoch 1523 | Loss: 0.036198 | LR: 4.5017e-04 | Grad Norm: 0.1162\n",
      "Epoch 1524 | Loss: 0.039533 | LR: 4.4984e-04 | Grad Norm: 0.0919\n",
      "Epoch 1525 | Loss: 0.041008 | LR: 4.4951e-04 | Grad Norm: 0.0838\n",
      "Epoch 1526 | Loss: 0.051409 | LR: 4.4918e-04 | Grad Norm: 0.1375\n",
      "Epoch 1527 | Loss: 0.093596 | LR: 4.4884e-04 | Grad Norm: 0.1879\n",
      "Epoch 1528 | Loss: 0.070803 | LR: 4.4851e-04 | Grad Norm: 0.1559\n",
      "Epoch 1529 | Loss: 0.084135 | LR: 4.4817e-04 | Grad Norm: 0.1583\n",
      "Epoch 1530 | Loss: 0.069739 | LR: 4.4784e-04 | Grad Norm: 0.1423\n",
      "Epoch 1531 | Loss: 0.020922 | LR: 4.4751e-04 | Grad Norm: 0.0496\n",
      "Epoch 1532 | Loss: 0.045694 | LR: 4.4717e-04 | Grad Norm: 0.1075\n",
      "Epoch 1533 | Loss: 0.083533 | LR: 4.4684e-04 | Grad Norm: 0.1974\n",
      "Epoch 1534 | Loss: 0.066981 | LR: 4.4650e-04 | Grad Norm: 0.1492\n",
      "Epoch 1535 | Loss: 0.117239 | LR: 4.4617e-04 | Grad Norm: 0.2322\n",
      "Epoch 1536 | Loss: 0.101350 | LR: 4.4583e-04 | Grad Norm: 0.2694\n",
      "Epoch 1537 | Loss: 0.111986 | LR: 4.4550e-04 | Grad Norm: 0.2227\n",
      "Epoch 1538 | Loss: 0.065289 | LR: 4.4516e-04 | Grad Norm: 0.2037\n",
      "Epoch 1539 | Loss: 0.069559 | LR: 4.4483e-04 | Grad Norm: 0.2149\n",
      "Epoch 1540 | Loss: 0.071063 | LR: 4.4449e-04 | Grad Norm: 0.1537\n",
      "Epoch 1541 | Loss: 0.090356 | LR: 4.4415e-04 | Grad Norm: 0.2418\n",
      "Epoch 1542 | Loss: 0.076373 | LR: 4.4382e-04 | Grad Norm: 0.1778\n",
      "Epoch 1543 | Loss: 0.098881 | LR: 4.4348e-04 | Grad Norm: 0.2126\n",
      "Epoch 1544 | Loss: 0.028473 | LR: 4.4314e-04 | Grad Norm: 0.0788\n",
      "Epoch 1545 | Loss: 0.076091 | LR: 4.4281e-04 | Grad Norm: 0.1409\n",
      "Epoch 1546 | Loss: 0.080964 | LR: 4.4247e-04 | Grad Norm: 0.1650\n",
      "Epoch 1547 | Loss: 0.013770 | LR: 4.4213e-04 | Grad Norm: 0.0362\n",
      "Epoch 1548 | Loss: 0.096251 | LR: 4.4179e-04 | Grad Norm: 0.1923\n",
      "Epoch 1549 | Loss: 0.019776 | LR: 4.4146e-04 | Grad Norm: 0.0685\n",
      "Epoch 1550 | Loss: 0.118146 | LR: 4.4112e-04 | Grad Norm: 0.2036\n",
      "Epoch 1551 | Loss: 0.115049 | LR: 4.4078e-04 | Grad Norm: 0.2076\n",
      "Epoch 1552 | Loss: 0.093718 | LR: 4.4044e-04 | Grad Norm: 0.1579\n",
      "Epoch 1553 | Loss: 0.023086 | LR: 4.4010e-04 | Grad Norm: 0.1353\n",
      "Epoch 1554 | Loss: 0.015790 | LR: 4.3976e-04 | Grad Norm: 0.0446\n",
      "Epoch 1555 | Loss: 0.103071 | LR: 4.3942e-04 | Grad Norm: 0.2275\n",
      "Epoch 1556 | Loss: 0.049698 | LR: 4.3908e-04 | Grad Norm: 0.7454\n",
      "Epoch 1557 | Loss: 0.107357 | LR: 4.3874e-04 | Grad Norm: 0.1889\n",
      "Epoch 1558 | Loss: 0.040139 | LR: 4.3840e-04 | Grad Norm: 0.1741\n",
      "Epoch 1559 | Loss: 0.170928 | LR: 4.3806e-04 | Grad Norm: 0.2795\n",
      "Epoch 1560 | Loss: 0.032991 | LR: 4.3772e-04 | Grad Norm: 0.1415\n",
      "Epoch 1561 | Loss: 0.120394 | LR: 4.3738e-04 | Grad Norm: 0.2635\n",
      "Epoch 1562 | Loss: 0.009792 | LR: 4.3704e-04 | Grad Norm: 0.0725\n",
      "Epoch 1563 | Loss: 0.008685 | LR: 4.3670e-04 | Grad Norm: 0.0442\n",
      "Epoch 1564 | Loss: 0.007228 | LR: 4.3636e-04 | Grad Norm: 0.0523\n",
      "Epoch 1565 | Loss: 0.093276 | LR: 4.3602e-04 | Grad Norm: 0.1828\n",
      "Epoch 1566 | Loss: 0.062155 | LR: 4.3568e-04 | Grad Norm: 0.1958\n",
      "Epoch 1567 | Loss: 0.094847 | LR: 4.3534e-04 | Grad Norm: 0.1814\n",
      "Epoch 1568 | Loss: 0.128920 | LR: 4.3499e-04 | Grad Norm: 0.3339\n",
      "Epoch 1569 | Loss: 0.010720 | LR: 4.3465e-04 | Grad Norm: 0.0894\n",
      "Epoch 1570 | Loss: 0.019368 | LR: 4.3431e-04 | Grad Norm: 0.1009\n",
      "Epoch 1571 | Loss: 0.009191 | LR: 4.3397e-04 | Grad Norm: 0.0403\n",
      "Epoch 1572 | Loss: 0.009676 | LR: 4.3362e-04 | Grad Norm: 0.0512\n",
      "Epoch 1573 | Loss: 0.014609 | LR: 4.3328e-04 | Grad Norm: 0.1882\n",
      "Epoch 1574 | Loss: 0.009767 | LR: 4.3294e-04 | Grad Norm: 0.0801\n",
      "Epoch 1575 | Loss: 0.152851 | LR: 4.3260e-04 | Grad Norm: 0.3260\n",
      "Epoch 1576 | Loss: 0.120785 | LR: 4.3225e-04 | Grad Norm: 0.2989\n",
      "Epoch 1577 | Loss: 0.163017 | LR: 4.3191e-04 | Grad Norm: 0.3376\n",
      "Epoch 1578 | Loss: 0.007721 | LR: 4.3156e-04 | Grad Norm: 0.0260\n",
      "Epoch 1579 | Loss: 0.123214 | LR: 4.3122e-04 | Grad Norm: 0.2582\n",
      "Epoch 1580 | Loss: 0.105014 | LR: 4.3088e-04 | Grad Norm: 0.2230\n",
      "Epoch 1581 | Loss: 0.044776 | LR: 4.3053e-04 | Grad Norm: 0.1887\n",
      "Epoch 1582 | Loss: 0.080143 | LR: 4.3019e-04 | Grad Norm: 0.8079\n",
      "Epoch 1583 | Loss: 0.159247 | LR: 4.2984e-04 | Grad Norm: 0.3354\n",
      "Epoch 1584 | Loss: 0.152296 | LR: 4.2950e-04 | Grad Norm: 0.2757\n",
      "Epoch 1585 | Loss: 0.163113 | LR: 4.2915e-04 | Grad Norm: 0.6873\n",
      "Epoch 1586 | Loss: 0.010066 | LR: 4.2881e-04 | Grad Norm: 0.1297\n",
      "Epoch 1587 | Loss: 0.139043 | LR: 4.2846e-04 | Grad Norm: 0.3426\n",
      "Epoch 1588 | Loss: 0.098803 | LR: 4.2812e-04 | Grad Norm: 0.2300\n",
      "Epoch 1589 | Loss: 0.013059 | LR: 4.2777e-04 | Grad Norm: 0.0819\n",
      "Epoch 1590 | Loss: 0.076387 | LR: 4.2742e-04 | Grad Norm: 0.1769\n",
      "Epoch 1591 | Loss: 0.091659 | LR: 4.2708e-04 | Grad Norm: 0.2088\n",
      "Epoch 1592 | Loss: 0.145977 | LR: 4.2673e-04 | Grad Norm: 0.2512\n",
      "Epoch 1593 | Loss: 0.081685 | LR: 4.2638e-04 | Grad Norm: 0.1734\n",
      "Epoch 1594 | Loss: 0.077138 | LR: 4.2604e-04 | Grad Norm: 0.5197\n",
      "Epoch 1595 | Loss: 0.094759 | LR: 4.2569e-04 | Grad Norm: 0.2405\n",
      "Epoch 1596 | Loss: 0.097580 | LR: 4.2534e-04 | Grad Norm: 0.1979\n",
      "Epoch 1597 | Loss: 0.115994 | LR: 4.2500e-04 | Grad Norm: 0.3485\n",
      "Epoch 1598 | Loss: 0.012783 | LR: 4.2465e-04 | Grad Norm: 0.1431\n",
      "Epoch 1599 | Loss: 0.063313 | LR: 4.2430e-04 | Grad Norm: 0.1674\n",
      "Epoch 1600 | Loss: 0.078247 | LR: 4.2395e-04 | Grad Norm: 0.4308\n"
     ]
    }
   ],
   "source": [
    "from encoder import tokenizer as t1,m as m_encode\n",
    "from decoder_2 import tokenizer as t2,m as m_decode\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "import inspect\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 2000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"Wait! Who is coming here? \"\n",
    "source_texted=[source_text+\" <EOS>\"]\n",
    "seqqed=t1.texts_to_sequences(source_texted)\n",
    "\n",
    "padsequences = pad_sequences(seqqed, maxlen=64, padding='pre')\n",
    "source_tokens = torch.tensor(padsequences, dtype=torch.long).to(\"cuda\")  # (1, T)\n",
    "# Encoder çıkışı (kaynak metnin bağlam temsilini çıkarıyoruz)\n",
    "with torch.no_grad():\n",
    "    source_embeddings = m_encode(source_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = torch.tensor(t2.texts_to_sequences([\"<BOS>\"]), dtype=torch.long).to(\"cuda\")  # <BOS> ile başlat\n",
    "generated = target_seq.clone()\n",
    "stoi = t2.word_index\n",
    "itos = dict(zip(stoi.values(), stoi.keys()))\n",
    "vocab_size = len(stoi)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, source_embeddings, max_new_tokens=50, bos_token=\"<BOS>\", eos_token=\"<EOS>\"):\n",
    "    model.eval()  # Modeli değerlendirme moduna al\n",
    "    \n",
    "    # Başlangıç tokenini tensor formatına çevir\n",
    "    bos_idx = torch.tensor(t2.texts_to_sequences([bos_token])[0], device=device).unsqueeze(0)  # (1, 1)\n",
    "    \n",
    "    # Üretilen diziyi başlat\n",
    "    generated = bos_idx  # (1, 1)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Modelden çıktı al\n",
    "        logits = model(generated, source_embeddings)\n",
    "          # (1, T, vocab_size)\n",
    "        logits = logits[:, -1, :]  # Sadece son adımı al (1, vocab_size)\n",
    "        \n",
    "        # Olasılıkları softmax ile hesapla\n",
    "        probs = F.softmax(logits, dim=-1)/0.7  # (1, vocab_size)\n",
    "       \n",
    "        # Token seçimi (argmax veya sampling)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # (1, 1)\n",
    "        \n",
    "        # Yeni tokeni ekle\n",
    "        generated = torch.cat([generated, next_token], dim=1)  # (1, T+1)\n",
    "        \n",
    "        # Eğer EOS tokenine ulaşıldıysa, döngüyü kır\n",
    "        if next_token.item() == t2.texts_to_sequences([eos_token])[0][0]:\n",
    "            break\n",
    "\n",
    "    return generated  # (1, T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=generate(m_decode,source_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 302,  53,  26, 324, 128,   2]], device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(result):\n",
    "    eleman=\"\"\n",
    "    for i in result[0]:\n",
    "        if(i==1 or i==2):\n",
    "            continue\n",
    "        eleman+=\" \"+itos[int(i)]\n",
    "    return eleman\n",
    "translated=translate(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " orada patrikler onlara tefeciliği pahalı\n"
     ]
    }
   ],
   "source": [
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "altantorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
